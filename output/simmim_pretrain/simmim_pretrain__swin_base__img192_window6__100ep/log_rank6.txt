[2022-04-02 13:37:45 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 13:37:45 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f61e14276a0>
[2022-04-02 13:37:49 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 13:37:49 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 13:37:50 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24, 24, 24), dim=192
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12, 12, 12), dim=384
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6, 6, 6), dim=768
          (reduction): Linear(in_features=3072, out_features=1536, bias=False)
          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder): Sequential(
    (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-04-02 13:37:50 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 13:37:50 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.mask_token', 'encoder.absolute_pos_embed'}
[2022-04-02 13:37:50 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 13:37:50 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'decoder.0.bias']
[2022-04-02 13:37:50 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder.0.weight']
[2022-04-02 13:37:50 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 13:37:50 simmim_pretrain] (main_simmim.py 83): INFO number of params: 34525434
[2022-04-02 13:37:50 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 13:37:50 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 13:37:50 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 13:38:32 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 13:38:32 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f40ba0876a0>
[2022-04-02 13:38:37 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 13:38:37 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 13:38:37 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=3072, out_features=1536, bias=False)
          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder): Sequential(
    (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-04-02 13:38:37 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 13:38:37 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.mask_token', 'encoder.absolute_pos_embed'}
[2022-04-02 13:38:37 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 13:38:37 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'decoder.0.bias']
[2022-04-02 13:38:37 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder.0.weight']
[2022-04-02 13:38:37 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 13:38:37 simmim_pretrain] (main_simmim.py 83): INFO number of params: 34525434
[2022-04-02 13:38:38 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 13:38:38 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 13:38:38 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 13:39:32 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 13:39:32 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7efdefb206a0>
[2022-04-02 13:39:37 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 13:39:37 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 13:39:37 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=3072, out_features=1536, bias=False)
          (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder): Sequential(
    (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-04-02 13:39:37 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 13:39:37 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.absolute_pos_embed', 'encoder.mask_token'}
[2022-04-02 13:39:37 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 13:39:37 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'decoder.0.bias']
[2022-04-02 13:39:37 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder.0.weight']
[2022-04-02 13:39:37 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 13:39:37 simmim_pretrain] (main_simmim.py 83): INFO number of params: 34525434
[2022-04-02 13:39:37 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 13:39:37 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 13:39:37 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 13:44:27 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 13:44:27 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7fc1fd1216a0>
[2022-04-02 13:44:31 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 13:44:31 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 13:44:32 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder): Sequential(
    (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-04-02 13:44:32 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 13:44:32 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.mask_token', 'encoder.absolute_pos_embed'}
[2022-04-02 13:44:32 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 13:44:32 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.weight', 'encoder.norm.bias', 'decoder.0.bias']
[2022-04-02 13:44:32 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder.0.weight']
[2022-04-02 13:44:32 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 13:44:32 simmim_pretrain] (main_simmim.py 83): INFO number of params: 29875194
[2022-04-02 13:44:32 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 13:44:32 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 13:44:32 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 13:44:41 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/100][0/625]	eta 1:31:10 lr 0.000004	time 8.7522 (8.7522)	loss 1.1019 (1.1019)	grad_norm 0.5530 (0.5530)	mem 14822MB
[2022-04-02 13:45:43 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/100][100/625]	eta 0:06:07 lr 0.000017	time 0.6151 (0.7003)	loss 0.8223 (0.9523)	grad_norm 3.1254 (1.6526)	mem 15168MB
[2022-04-02 13:51:20 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 13:51:20 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f019c6ec6d0>
[2022-04-02 13:51:25 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 13:51:25 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 13:51:25 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): ModuleList(
      (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder): Sequential(
    (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-04-02 13:51:25 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 13:51:25 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.mask_token', 'encoder.absolute_pos_embed'}
[2022-04-02 13:51:25 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 13:51:25 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.0.weight', 'encoder.norm.0.bias', 'encoder.norm.1.weight', 'encoder.norm.1.bias', 'encoder.norm.2.weight', 'encoder.norm.2.bias', 'encoder.norm.3.weight', 'encoder.norm.3.bias', 'decoder.0.bias']
[2022-04-02 13:51:25 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder.0.weight']
[2022-04-02 13:51:25 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 13:51:25 simmim_pretrain] (main_simmim.py 83): INFO number of params: 29876538
[2022-04-02 13:51:25 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 13:51:25 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 13:51:25 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 13:52:11 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 13:52:11 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7ff80b45b6d0>
[2022-04-02 13:52:15 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 13:52:15 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 13:52:16 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): ModuleList(
      (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder): Sequential(
    (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-04-02 13:52:16 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 13:52:16 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.absolute_pos_embed', 'encoder.mask_token'}
[2022-04-02 13:52:16 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 13:52:16 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.0.weight', 'encoder.norm.0.bias', 'encoder.norm.1.weight', 'encoder.norm.1.bias', 'encoder.norm.2.weight', 'encoder.norm.2.bias', 'encoder.norm.3.weight', 'encoder.norm.3.bias', 'decoder.0.bias']
[2022-04-02 13:52:16 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder.0.weight']
[2022-04-02 13:52:16 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 13:52:16 simmim_pretrain] (main_simmim.py 83): INFO number of params: 29876538
[2022-04-02 13:52:16 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 13:52:16 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 13:52:16 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 13:52:24 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/100][0/625]	eta 1:22:52 lr 0.000004	time 7.9556 (7.9556)	loss 1.1019 (1.1019)	grad_norm 0.5466 (0.5466)	mem 15307MB
[2022-04-02 13:53:12 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 13:53:12 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7fd6baf266d0>
[2022-04-02 13:53:17 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 13:53:17 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 13:53:17 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): ModuleList(
      (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder): Sequential(
    (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-04-02 13:53:17 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 13:53:17 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.absolute_pos_embed', 'encoder.mask_token'}
[2022-04-02 13:53:17 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 13:53:17 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.0.weight', 'encoder.norm.0.bias', 'encoder.norm.1.weight', 'encoder.norm.1.bias', 'encoder.norm.2.weight', 'encoder.norm.2.bias', 'encoder.norm.3.weight', 'encoder.norm.3.bias', 'decoder.0.bias']
[2022-04-02 13:53:17 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder.0.weight']
[2022-04-02 13:53:17 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 13:53:17 simmim_pretrain] (main_simmim.py 83): INFO number of params: 29876538
[2022-04-02 13:53:17 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 13:53:17 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 13:53:17 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 13:53:25 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/100][0/625]	eta 1:25:06 lr 0.000004	time 8.1708 (8.1708)	loss 1.1019 (1.1019)	grad_norm 0.5530 (0.5530)	mem 15307MB
[2022-04-02 13:56:06 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 13:56:06 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f2c13935f70>
[2022-04-02 13:56:10 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 13:56:10 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 13:56:11 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): ModuleList(
      (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder): Sequential(
    (0): Conv2d(768, 3072, kernel_size=(1, 1), stride=(1, 1))
    (1): PixelShuffle(upscale_factor=32)
  )
)
[2022-04-02 13:56:11 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 13:56:11 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.absolute_pos_embed', 'encoder.mask_token'}
[2022-04-02 13:56:11 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 13:56:11 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.0.weight', 'encoder.norm.0.bias', 'encoder.norm.1.weight', 'encoder.norm.1.bias', 'encoder.norm.2.weight', 'encoder.norm.2.bias', 'encoder.norm.3.weight', 'encoder.norm.3.bias', 'decoder.0.bias']
[2022-04-02 13:56:11 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder.0.weight']
[2022-04-02 13:56:11 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 13:56:11 simmim_pretrain] (main_simmim.py 83): INFO number of params: 29876538
[2022-04-02 13:56:11 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 13:56:11 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 13:56:11 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 13:56:19 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/100][0/625]	eta 1:27:30 lr 0.000004	time 8.4013 (8.4013)	loss 1.1019 (1.1019)	grad_norm 0.5530 (0.5530)	mem 15307MB
[2022-04-02 13:58:44 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 13:58:44 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f9940b24730>
[2022-04-02 13:58:48 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 13:58:48 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 13:58:49 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): ModuleList(
      (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder_stage_4): Conv2d(768, 27, kernel_size=(1, 1), stride=(1, 1))
)
[2022-04-02 13:58:49 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 13:58:49 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.absolute_pos_embed', 'encoder.mask_token'}
[2022-04-02 13:58:49 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 13:58:49 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.0.weight', 'encoder.norm.0.bias', 'encoder.norm.1.weight', 'encoder.norm.1.bias', 'encoder.norm.2.weight', 'encoder.norm.2.bias', 'encoder.norm.3.weight', 'encoder.norm.3.bias', 'decoder_stage_4.bias']
[2022-04-02 13:58:49 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder_stage_4.weight']
[2022-04-02 13:58:49 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 13:58:49 simmim_pretrain] (main_simmim.py 83): INFO number of params: 27534933
[2022-04-02 13:58:49 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 13:58:49 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 13:58:49 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 13:59:24 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 13:59:24 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7fc975316730>
[2022-04-02 13:59:29 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 13:59:29 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 13:59:29 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): ModuleList(
      (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder_stage_4): Conv2d(768, 27, kernel_size=(1, 1), stride=(1, 1))
)
[2022-04-02 13:59:29 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 13:59:29 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.mask_token', 'encoder.absolute_pos_embed'}
[2022-04-02 13:59:29 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 13:59:29 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.0.weight', 'encoder.norm.0.bias', 'encoder.norm.1.weight', 'encoder.norm.1.bias', 'encoder.norm.2.weight', 'encoder.norm.2.bias', 'encoder.norm.3.weight', 'encoder.norm.3.bias', 'decoder_stage_4.bias']
[2022-04-02 13:59:29 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder_stage_4.weight']
[2022-04-02 13:59:29 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 13:59:29 simmim_pretrain] (main_simmim.py 83): INFO number of params: 27534933
[2022-04-02 13:59:29 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 13:59:29 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 13:59:29 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 14:00:29 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 14:00:29 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7fc752bbea90>
[2022-04-02 14:00:34 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 14:00:34 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 14:00:34 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): ModuleList(
      (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder_stage_4): Conv2d(768, 27, kernel_size=(1, 1), stride=(1, 1))
)
[2022-04-02 14:00:34 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 14:00:34 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.mask_token', 'encoder.absolute_pos_embed'}
[2022-04-02 14:00:34 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 14:00:34 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.0.weight', 'encoder.norm.0.bias', 'encoder.norm.1.weight', 'encoder.norm.1.bias', 'encoder.norm.2.weight', 'encoder.norm.2.bias', 'encoder.norm.3.weight', 'encoder.norm.3.bias', 'decoder_stage_4.bias']
[2022-04-02 14:00:34 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder_stage_4.weight']
[2022-04-02 14:00:34 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 14:00:34 simmim_pretrain] (main_simmim.py 83): INFO number of params: 27534933
[2022-04-02 14:00:34 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 14:00:34 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 14:00:34 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 14:01:26 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 14:01:26 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f98d0189a90>
[2022-04-02 14:01:31 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 14:01:31 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 14:01:31 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): ModuleList(
      (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder_stage_4): Conv2d(768, 27, kernel_size=(1, 1), stride=(1, 1))
)
[2022-04-02 14:01:31 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 14:01:31 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.mask_token', 'encoder.absolute_pos_embed'}
[2022-04-02 14:01:31 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 14:01:31 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.0.weight', 'encoder.norm.0.bias', 'encoder.norm.1.weight', 'encoder.norm.1.bias', 'encoder.norm.2.weight', 'encoder.norm.2.bias', 'encoder.norm.3.weight', 'encoder.norm.3.bias', 'decoder_stage_4.bias']
[2022-04-02 14:01:31 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder_stage_4.weight']
[2022-04-02 14:01:31 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 14:01:31 simmim_pretrain] (main_simmim.py 83): INFO number of params: 27534933
[2022-04-02 14:01:31 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 14:01:31 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 14:01:31 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 14:03:38 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 14:03:38 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f743ed6da90>
[2022-04-02 14:03:42 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 14:03:42 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 14:03:43 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): ModuleList(
      (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder_stage_4): Conv2d(768, 27, kernel_size=(1, 1), stride=(1, 1))
)
[2022-04-02 14:03:43 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 14:03:43 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.absolute_pos_embed', 'encoder.mask_token'}
[2022-04-02 14:03:43 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 14:03:43 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.0.weight', 'encoder.norm.0.bias', 'encoder.norm.1.weight', 'encoder.norm.1.bias', 'encoder.norm.2.weight', 'encoder.norm.2.bias', 'encoder.norm.3.weight', 'encoder.norm.3.bias', 'decoder_stage_4.bias']
[2022-04-02 14:03:43 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder_stage_4.weight']
[2022-04-02 14:03:43 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 14:03:43 simmim_pretrain] (main_simmim.py 83): INFO number of params: 27534933
[2022-04-02 14:03:43 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 14:03:43 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 14:03:43 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 14:05:53 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 14:05:53 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f996600eac0>
[2022-04-02 14:05:58 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 14:05:58 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 14:05:58 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): ModuleList(
      (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder_stage_4): Conv2d(768, 27, kernel_size=(1, 1), stride=(1, 1))
)
[2022-04-02 14:05:58 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 14:05:58 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.mask_token', 'encoder.absolute_pos_embed'}
[2022-04-02 14:05:58 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 14:05:58 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.0.weight', 'encoder.norm.0.bias', 'encoder.norm.1.weight', 'encoder.norm.1.bias', 'encoder.norm.2.weight', 'encoder.norm.2.bias', 'encoder.norm.3.weight', 'encoder.norm.3.bias', 'decoder_stage_4.bias']
[2022-04-02 14:05:58 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder_stage_4.weight']
[2022-04-02 14:05:58 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 14:05:58 simmim_pretrain] (main_simmim.py 83): INFO number of params: 27534933
[2022-04-02 14:05:58 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 14:05:58 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 14:05:58 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 14:06:06 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/100][0/625]	eta 1:20:58 lr 0.000004	time 7.7744 (7.7744)	loss 4.9201 (4.9201)	grad_norm inf (inf)	mem 15970MB
[2022-04-02 14:14:10 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 14:14:10 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f6d67311a90>
[2022-04-02 14:14:15 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 14:14:15 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 14:14:15 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): ModuleList(
      (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder_stage_4): Conv2d(768, 27, kernel_size=(1, 1), stride=(1, 1))
  (net): BasicLayer(
    dim=96, input_resolution=[24, 24], depth=6
    (blocks): ModuleList(
      (0): SwinTransformerBlock(
        dim=96, input_resolution=[24, 24], num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=96, window_size=(6, 6), num_heads=3
          (qkv): Linear(in_features=96, out_features=288, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=96, out_features=96, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): SwinTransformerBlock(
        dim=96, input_resolution=[24, 24], num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=96, window_size=(6, 6), num_heads=3
          (qkv): Linear(in_features=96, out_features=288, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=96, out_features=96, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): SwinTransformerBlock(
        dim=96, input_resolution=[24, 24], num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=96, window_size=(6, 6), num_heads=3
          (qkv): Linear(in_features=96, out_features=288, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=96, out_features=96, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): SwinTransformerBlock(
        dim=96, input_resolution=[24, 24], num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=96, window_size=(6, 6), num_heads=3
          (qkv): Linear(in_features=96, out_features=288, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=96, out_features=96, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): SwinTransformerBlock(
        dim=96, input_resolution=[24, 24], num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=96, window_size=(6, 6), num_heads=3
          (qkv): Linear(in_features=96, out_features=288, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=96, out_features=96, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): SwinTransformerBlock(
        dim=96, input_resolution=[24, 24], num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=96, window_size=(6, 6), num_heads=3
          (qkv): Linear(in_features=96, out_features=288, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=96, out_features=96, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=384, out_features=96, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
)
[2022-04-02 14:14:15 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 14:14:15 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.absolute_pos_embed', 'encoder.mask_token'}
[2022-04-02 14:14:15 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 14:14:15 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.0.weight', 'encoder.norm.0.bias', 'encoder.norm.1.weight', 'encoder.norm.1.bias', 'encoder.norm.2.weight', 'encoder.norm.2.bias', 'encoder.norm.3.weight', 'encoder.norm.3.bias', 'decoder_stage_4.bias', 'net.blocks.0.norm1.weight', 'net.blocks.0.norm1.bias', 'net.blocks.0.attn.qkv.bias', 'net.blocks.0.attn.proj.bias', 'net.blocks.0.norm2.weight', 'net.blocks.0.norm2.bias', 'net.blocks.0.mlp.fc1.bias', 'net.blocks.0.mlp.fc2.bias', 'net.blocks.1.norm1.weight', 'net.blocks.1.norm1.bias', 'net.blocks.1.attn.qkv.bias', 'net.blocks.1.attn.proj.bias', 'net.blocks.1.norm2.weight', 'net.blocks.1.norm2.bias', 'net.blocks.1.mlp.fc1.bias', 'net.blocks.1.mlp.fc2.bias', 'net.blocks.2.norm1.weight', 'net.blocks.2.norm1.bias', 'net.blocks.2.attn.qkv.bias', 'net.blocks.2.attn.proj.bias', 'net.blocks.2.norm2.weight', 'net.blocks.2.norm2.bias', 'net.blocks.2.mlp.fc1.bias', 'net.blocks.2.mlp.fc2.bias', 'net.blocks.3.norm1.weight', 'net.blocks.3.norm1.bias', 'net.blocks.3.attn.qkv.bias', 'net.blocks.3.attn.proj.bias', 'net.blocks.3.norm2.weight', 'net.blocks.3.norm2.bias', 'net.blocks.3.mlp.fc1.bias', 'net.blocks.3.mlp.fc2.bias', 'net.blocks.4.norm1.weight', 'net.blocks.4.norm1.bias', 'net.blocks.4.attn.qkv.bias', 'net.blocks.4.attn.proj.bias', 'net.blocks.4.norm2.weight', 'net.blocks.4.norm2.bias', 'net.blocks.4.mlp.fc1.bias', 'net.blocks.4.mlp.fc2.bias', 'net.blocks.5.norm1.weight', 'net.blocks.5.norm1.bias', 'net.blocks.5.attn.qkv.bias', 'net.blocks.5.attn.proj.bias', 'net.blocks.5.norm2.weight', 'net.blocks.5.norm2.bias', 'net.blocks.5.mlp.fc1.bias', 'net.blocks.5.mlp.fc2.bias']
[2022-04-02 14:14:15 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder_stage_4.weight', 'net.blocks.0.attn.relative_position_bias_table', 'net.blocks.0.attn.qkv.weight', 'net.blocks.0.attn.proj.weight', 'net.blocks.0.mlp.fc1.weight', 'net.blocks.0.mlp.fc2.weight', 'net.blocks.1.attn.relative_position_bias_table', 'net.blocks.1.attn.qkv.weight', 'net.blocks.1.attn.proj.weight', 'net.blocks.1.mlp.fc1.weight', 'net.blocks.1.mlp.fc2.weight', 'net.blocks.2.attn.relative_position_bias_table', 'net.blocks.2.attn.qkv.weight', 'net.blocks.2.attn.proj.weight', 'net.blocks.2.mlp.fc1.weight', 'net.blocks.2.mlp.fc2.weight', 'net.blocks.3.attn.relative_position_bias_table', 'net.blocks.3.attn.qkv.weight', 'net.blocks.3.attn.proj.weight', 'net.blocks.3.mlp.fc1.weight', 'net.blocks.3.mlp.fc2.weight', 'net.blocks.4.attn.relative_position_bias_table', 'net.blocks.4.attn.qkv.weight', 'net.blocks.4.attn.proj.weight', 'net.blocks.4.mlp.fc1.weight', 'net.blocks.4.mlp.fc2.weight', 'net.blocks.5.attn.relative_position_bias_table', 'net.blocks.5.attn.qkv.weight', 'net.blocks.5.attn.proj.weight', 'net.blocks.5.mlp.fc1.weight', 'net.blocks.5.mlp.fc2.weight']
[2022-04-02 14:14:15 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 14:14:15 simmim_pretrain] (main_simmim.py 83): INFO number of params: 28208151
[2022-04-02 14:14:15 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 14:14:15 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 14:14:15 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 14:14:23 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/100][0/625]	eta 1:23:37 lr 0.000004	time 8.0279 (8.0279)	loss 4.9265 (4.9265)	grad_norm inf (inf)	mem 15977MB
[2022-04-02 17:58:58 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 17:58:58 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f62bfcedac0>
[2022-04-02 17:59:03 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 17:59:03 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 17:59:03 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): ModuleList(
      (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder_stage_4): Conv2d(768, 27, kernel_size=(1, 1), stride=(1, 1))
  (decoder_stage_2): Conv2d(192, 27, kernel_size=(1, 1), stride=(1, 1))
  (neck): BasicLayer(
    dim=192, input_resolution=[12, 12], depth=6
    (blocks): ModuleList(
      (0): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
)
[2022-04-02 17:59:03 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 17:59:03 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.absolute_pos_embed', 'encoder.mask_token'}
[2022-04-02 17:59:03 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 17:59:03 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.0.weight', 'encoder.norm.0.bias', 'encoder.norm.1.weight', 'encoder.norm.1.bias', 'encoder.norm.2.weight', 'encoder.norm.2.bias', 'encoder.norm.3.weight', 'encoder.norm.3.bias', 'decoder_stage_4.bias', 'decoder_stage_2.bias', 'neck.blocks.0.norm1.weight', 'neck.blocks.0.norm1.bias', 'neck.blocks.0.attn.qkv.bias', 'neck.blocks.0.attn.proj.bias', 'neck.blocks.0.norm2.weight', 'neck.blocks.0.norm2.bias', 'neck.blocks.0.mlp.fc1.bias', 'neck.blocks.0.mlp.fc2.bias', 'neck.blocks.1.norm1.weight', 'neck.blocks.1.norm1.bias', 'neck.blocks.1.attn.qkv.bias', 'neck.blocks.1.attn.proj.bias', 'neck.blocks.1.norm2.weight', 'neck.blocks.1.norm2.bias', 'neck.blocks.1.mlp.fc1.bias', 'neck.blocks.1.mlp.fc2.bias', 'neck.blocks.2.norm1.weight', 'neck.blocks.2.norm1.bias', 'neck.blocks.2.attn.qkv.bias', 'neck.blocks.2.attn.proj.bias', 'neck.blocks.2.norm2.weight', 'neck.blocks.2.norm2.bias', 'neck.blocks.2.mlp.fc1.bias', 'neck.blocks.2.mlp.fc2.bias', 'neck.blocks.3.norm1.weight', 'neck.blocks.3.norm1.bias', 'neck.blocks.3.attn.qkv.bias', 'neck.blocks.3.attn.proj.bias', 'neck.blocks.3.norm2.weight', 'neck.blocks.3.norm2.bias', 'neck.blocks.3.mlp.fc1.bias', 'neck.blocks.3.mlp.fc2.bias', 'neck.blocks.4.norm1.weight', 'neck.blocks.4.norm1.bias', 'neck.blocks.4.attn.qkv.bias', 'neck.blocks.4.attn.proj.bias', 'neck.blocks.4.norm2.weight', 'neck.blocks.4.norm2.bias', 'neck.blocks.4.mlp.fc1.bias', 'neck.blocks.4.mlp.fc2.bias', 'neck.blocks.5.norm1.weight', 'neck.blocks.5.norm1.bias', 'neck.blocks.5.attn.qkv.bias', 'neck.blocks.5.attn.proj.bias', 'neck.blocks.5.norm2.weight', 'neck.blocks.5.norm2.bias', 'neck.blocks.5.mlp.fc1.bias', 'neck.blocks.5.mlp.fc2.bias']
[2022-04-02 17:59:03 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder_stage_4.weight', 'decoder_stage_2.weight', 'neck.blocks.0.attn.relative_position_bias_table', 'neck.blocks.0.attn.qkv.weight', 'neck.blocks.0.attn.proj.weight', 'neck.blocks.0.mlp.fc1.weight', 'neck.blocks.0.mlp.fc2.weight', 'neck.blocks.1.attn.relative_position_bias_table', 'neck.blocks.1.attn.qkv.weight', 'neck.blocks.1.attn.proj.weight', 'neck.blocks.1.mlp.fc1.weight', 'neck.blocks.1.mlp.fc2.weight', 'neck.blocks.2.attn.relative_position_bias_table', 'neck.blocks.2.attn.qkv.weight', 'neck.blocks.2.attn.proj.weight', 'neck.blocks.2.mlp.fc1.weight', 'neck.blocks.2.mlp.fc2.weight', 'neck.blocks.3.attn.relative_position_bias_table', 'neck.blocks.3.attn.qkv.weight', 'neck.blocks.3.attn.proj.weight', 'neck.blocks.3.mlp.fc1.weight', 'neck.blocks.3.mlp.fc2.weight', 'neck.blocks.4.attn.relative_position_bias_table', 'neck.blocks.4.attn.qkv.weight', 'neck.blocks.4.attn.proj.weight', 'neck.blocks.4.mlp.fc1.weight', 'neck.blocks.4.mlp.fc2.weight', 'neck.blocks.5.attn.relative_position_bias_table', 'neck.blocks.5.attn.qkv.weight', 'neck.blocks.5.attn.proj.weight', 'neck.blocks.5.mlp.fc1.weight', 'neck.blocks.5.mlp.fc2.weight']
[2022-04-02 17:59:03 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 17:59:03 simmim_pretrain] (main_simmim.py 83): INFO number of params: 30218040
[2022-04-02 17:59:03 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 17:59:03 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 17:59:03 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 18:01:18 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 18:01:18 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f3088e00a90>
[2022-04-02 18:01:22 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 18:01:22 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 18:01:23 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): ModuleList(
      (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder_stage_4): Conv2d(768, 27, kernel_size=(1, 1), stride=(1, 1))
  (decoder_stage_2): Conv2d(192, 27, kernel_size=(1, 1), stride=(1, 1))
  (down_sample): Linear(in_features=384, out_features=192, bias=True)
  (neck): BasicLayer(
    dim=192, input_resolution=[12, 12], depth=6
    (blocks): ModuleList(
      (0): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
)
[2022-04-02 18:01:23 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 18:01:23 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.absolute_pos_embed', 'encoder.mask_token'}
[2022-04-02 18:01:23 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 18:01:23 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.0.weight', 'encoder.norm.0.bias', 'encoder.norm.1.weight', 'encoder.norm.1.bias', 'encoder.norm.2.weight', 'encoder.norm.2.bias', 'encoder.norm.3.weight', 'encoder.norm.3.bias', 'decoder_stage_4.bias', 'decoder_stage_2.bias', 'down_sample.bias', 'neck.blocks.0.norm1.weight', 'neck.blocks.0.norm1.bias', 'neck.blocks.0.attn.qkv.bias', 'neck.blocks.0.attn.proj.bias', 'neck.blocks.0.norm2.weight', 'neck.blocks.0.norm2.bias', 'neck.blocks.0.mlp.fc1.bias', 'neck.blocks.0.mlp.fc2.bias', 'neck.blocks.1.norm1.weight', 'neck.blocks.1.norm1.bias', 'neck.blocks.1.attn.qkv.bias', 'neck.blocks.1.attn.proj.bias', 'neck.blocks.1.norm2.weight', 'neck.blocks.1.norm2.bias', 'neck.blocks.1.mlp.fc1.bias', 'neck.blocks.1.mlp.fc2.bias', 'neck.blocks.2.norm1.weight', 'neck.blocks.2.norm1.bias', 'neck.blocks.2.attn.qkv.bias', 'neck.blocks.2.attn.proj.bias', 'neck.blocks.2.norm2.weight', 'neck.blocks.2.norm2.bias', 'neck.blocks.2.mlp.fc1.bias', 'neck.blocks.2.mlp.fc2.bias', 'neck.blocks.3.norm1.weight', 'neck.blocks.3.norm1.bias', 'neck.blocks.3.attn.qkv.bias', 'neck.blocks.3.attn.proj.bias', 'neck.blocks.3.norm2.weight', 'neck.blocks.3.norm2.bias', 'neck.blocks.3.mlp.fc1.bias', 'neck.blocks.3.mlp.fc2.bias', 'neck.blocks.4.norm1.weight', 'neck.blocks.4.norm1.bias', 'neck.blocks.4.attn.qkv.bias', 'neck.blocks.4.attn.proj.bias', 'neck.blocks.4.norm2.weight', 'neck.blocks.4.norm2.bias', 'neck.blocks.4.mlp.fc1.bias', 'neck.blocks.4.mlp.fc2.bias', 'neck.blocks.5.norm1.weight', 'neck.blocks.5.norm1.bias', 'neck.blocks.5.attn.qkv.bias', 'neck.blocks.5.attn.proj.bias', 'neck.blocks.5.norm2.weight', 'neck.blocks.5.norm2.bias', 'neck.blocks.5.mlp.fc1.bias', 'neck.blocks.5.mlp.fc2.bias']
[2022-04-02 18:01:23 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder_stage_4.weight', 'decoder_stage_2.weight', 'down_sample.weight', 'neck.blocks.0.attn.relative_position_bias_table', 'neck.blocks.0.attn.qkv.weight', 'neck.blocks.0.attn.proj.weight', 'neck.blocks.0.mlp.fc1.weight', 'neck.blocks.0.mlp.fc2.weight', 'neck.blocks.1.attn.relative_position_bias_table', 'neck.blocks.1.attn.qkv.weight', 'neck.blocks.1.attn.proj.weight', 'neck.blocks.1.mlp.fc1.weight', 'neck.blocks.1.mlp.fc2.weight', 'neck.blocks.2.attn.relative_position_bias_table', 'neck.blocks.2.attn.qkv.weight', 'neck.blocks.2.attn.proj.weight', 'neck.blocks.2.mlp.fc1.weight', 'neck.blocks.2.mlp.fc2.weight', 'neck.blocks.3.attn.relative_position_bias_table', 'neck.blocks.3.attn.qkv.weight', 'neck.blocks.3.attn.proj.weight', 'neck.blocks.3.mlp.fc1.weight', 'neck.blocks.3.mlp.fc2.weight', 'neck.blocks.4.attn.relative_position_bias_table', 'neck.blocks.4.attn.qkv.weight', 'neck.blocks.4.attn.proj.weight', 'neck.blocks.4.mlp.fc1.weight', 'neck.blocks.4.mlp.fc2.weight', 'neck.blocks.5.attn.relative_position_bias_table', 'neck.blocks.5.attn.qkv.weight', 'neck.blocks.5.attn.proj.weight', 'neck.blocks.5.mlp.fc1.weight', 'neck.blocks.5.mlp.fc2.weight']
[2022-04-02 18:01:23 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 18:01:23 simmim_pretrain] (main_simmim.py 83): INFO number of params: 30291960
[2022-04-02 18:01:23 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 18:01:23 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 18:01:23 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 18:02:32 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-02 18:02:32 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7fc7dcca1a90>
[2022-04-02 18:02:36 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-02 18:02:36 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-02 18:02:37 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): ModuleList(
      (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder_stage_4): Conv2d(768, 27, kernel_size=(1, 1), stride=(1, 1))
  (decoder_stage_2): Conv2d(192, 27, kernel_size=(1, 1), stride=(1, 1))
  (down_sample): Linear(in_features=384, out_features=192, bias=True)
  (neck): BasicLayer(
    dim=192, input_resolution=[12, 12], depth=6
    (blocks): ModuleList(
      (0): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
)
[2022-04-02 18:02:37 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-02 18:02:37 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.mask_token', 'encoder.absolute_pos_embed'}
[2022-04-02 18:02:37 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-02 18:02:37 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.0.weight', 'encoder.norm.0.bias', 'encoder.norm.1.weight', 'encoder.norm.1.bias', 'encoder.norm.2.weight', 'encoder.norm.2.bias', 'encoder.norm.3.weight', 'encoder.norm.3.bias', 'decoder_stage_4.bias', 'decoder_stage_2.bias', 'down_sample.bias', 'neck.blocks.0.norm1.weight', 'neck.blocks.0.norm1.bias', 'neck.blocks.0.attn.qkv.bias', 'neck.blocks.0.attn.proj.bias', 'neck.blocks.0.norm2.weight', 'neck.blocks.0.norm2.bias', 'neck.blocks.0.mlp.fc1.bias', 'neck.blocks.0.mlp.fc2.bias', 'neck.blocks.1.norm1.weight', 'neck.blocks.1.norm1.bias', 'neck.blocks.1.attn.qkv.bias', 'neck.blocks.1.attn.proj.bias', 'neck.blocks.1.norm2.weight', 'neck.blocks.1.norm2.bias', 'neck.blocks.1.mlp.fc1.bias', 'neck.blocks.1.mlp.fc2.bias', 'neck.blocks.2.norm1.weight', 'neck.blocks.2.norm1.bias', 'neck.blocks.2.attn.qkv.bias', 'neck.blocks.2.attn.proj.bias', 'neck.blocks.2.norm2.weight', 'neck.blocks.2.norm2.bias', 'neck.blocks.2.mlp.fc1.bias', 'neck.blocks.2.mlp.fc2.bias', 'neck.blocks.3.norm1.weight', 'neck.blocks.3.norm1.bias', 'neck.blocks.3.attn.qkv.bias', 'neck.blocks.3.attn.proj.bias', 'neck.blocks.3.norm2.weight', 'neck.blocks.3.norm2.bias', 'neck.blocks.3.mlp.fc1.bias', 'neck.blocks.3.mlp.fc2.bias', 'neck.blocks.4.norm1.weight', 'neck.blocks.4.norm1.bias', 'neck.blocks.4.attn.qkv.bias', 'neck.blocks.4.attn.proj.bias', 'neck.blocks.4.norm2.weight', 'neck.blocks.4.norm2.bias', 'neck.blocks.4.mlp.fc1.bias', 'neck.blocks.4.mlp.fc2.bias', 'neck.blocks.5.norm1.weight', 'neck.blocks.5.norm1.bias', 'neck.blocks.5.attn.qkv.bias', 'neck.blocks.5.attn.proj.bias', 'neck.blocks.5.norm2.weight', 'neck.blocks.5.norm2.bias', 'neck.blocks.5.mlp.fc1.bias', 'neck.blocks.5.mlp.fc2.bias']
[2022-04-02 18:02:37 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder_stage_4.weight', 'decoder_stage_2.weight', 'down_sample.weight', 'neck.blocks.0.attn.relative_position_bias_table', 'neck.blocks.0.attn.qkv.weight', 'neck.blocks.0.attn.proj.weight', 'neck.blocks.0.mlp.fc1.weight', 'neck.blocks.0.mlp.fc2.weight', 'neck.blocks.1.attn.relative_position_bias_table', 'neck.blocks.1.attn.qkv.weight', 'neck.blocks.1.attn.proj.weight', 'neck.blocks.1.mlp.fc1.weight', 'neck.blocks.1.mlp.fc2.weight', 'neck.blocks.2.attn.relative_position_bias_table', 'neck.blocks.2.attn.qkv.weight', 'neck.blocks.2.attn.proj.weight', 'neck.blocks.2.mlp.fc1.weight', 'neck.blocks.2.mlp.fc2.weight', 'neck.blocks.3.attn.relative_position_bias_table', 'neck.blocks.3.attn.qkv.weight', 'neck.blocks.3.attn.proj.weight', 'neck.blocks.3.mlp.fc1.weight', 'neck.blocks.3.mlp.fc2.weight', 'neck.blocks.4.attn.relative_position_bias_table', 'neck.blocks.4.attn.qkv.weight', 'neck.blocks.4.attn.proj.weight', 'neck.blocks.4.mlp.fc1.weight', 'neck.blocks.4.mlp.fc2.weight', 'neck.blocks.5.attn.relative_position_bias_table', 'neck.blocks.5.attn.qkv.weight', 'neck.blocks.5.attn.proj.weight', 'neck.blocks.5.mlp.fc1.weight', 'neck.blocks.5.mlp.fc2.weight']
[2022-04-02 18:02:37 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-02 18:02:37 simmim_pretrain] (main_simmim.py 83): INFO number of params: 30291960
[2022-04-02 18:02:37 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: []
[2022-04-02 18:02:37 simmim_pretrain] (main_simmim.py 100): INFO no checkpoint found in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep, ignoring auto resume
[2022-04-02 18:02:37 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-02 18:02:45 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/100][0/625]	eta 1:26:22 lr 0.000004	time 8.2917 (8.2917)	loss 9.3995 (9.3995)	grad_norm inf (inf)	mem 19136MB
[2022-04-02 18:04:00 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/100][100/625]	eta 0:07:14 lr 0.000017	time 0.7637 (0.8273)	loss 2.6509 (3.3415)	grad_norm 8.0923 (inf)	mem 19488MB
[2022-04-02 18:05:17 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/100][200/625]	eta 0:05:38 lr 0.000029	time 0.7680 (0.7957)	loss 2.6614 (2.9891)	grad_norm 11.0702 (inf)	mem 19488MB
[2022-04-02 18:06:33 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/100][300/625]	eta 0:04:15 lr 0.000042	time 0.7668 (0.7863)	loss 2.6180 (2.8711)	grad_norm 15.3477 (inf)	mem 19488MB
[2022-04-02 18:07:50 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/100][400/625]	eta 0:02:55 lr 0.000055	time 0.7771 (0.7812)	loss 2.6038 (2.8020)	grad_norm 11.3354 (inf)	mem 19489MB
[2022-04-02 18:09:07 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/100][500/625]	eta 0:01:37 lr 0.000068	time 0.7670 (0.7786)	loss 2.5131 (2.7525)	grad_norm 13.4585 (inf)	mem 19489MB
[2022-04-02 18:10:24 simmim_pretrain] (main_simmim.py 184): INFO Train: [0/100][600/625]	eta 0:00:19 lr 0.000080	time 0.7759 (0.7767)	loss 2.4191 (2.7113)	grad_norm 12.5695 (inf)	mem 19489MB
[2022-04-02 18:10:42 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 0 training takes 0:08:05
[2022-04-02 18:10:47 simmim_pretrain] (main_simmim.py 184): INFO Train: [1/100][0/625]	eta 0:51:26 lr 0.000084	time 4.9384 (4.9384)	loss 2.4680 (2.4680)	grad_norm 12.4561 (12.4561)	mem 19489MB
[2022-04-02 18:12:04 simmim_pretrain] (main_simmim.py 184): INFO Train: [1/100][100/625]	eta 0:07:06 lr 0.000096	time 0.7613 (0.8115)	loss 2.4599 (2.4239)	grad_norm 19.6147 (14.3125)	mem 19489MB
[2022-04-02 18:13:21 simmim_pretrain] (main_simmim.py 184): INFO Train: [1/100][200/625]	eta 0:05:35 lr 0.000109	time 0.7704 (0.7894)	loss 2.4082 (2.4144)	grad_norm 17.3576 (14.8107)	mem 19489MB
[2022-04-02 18:14:38 simmim_pretrain] (main_simmim.py 184): INFO Train: [1/100][300/625]	eta 0:04:14 lr 0.000122	time 0.7580 (0.7819)	loss 2.3734 (2.4004)	grad_norm 12.7537 (14.5966)	mem 19489MB
[2022-04-02 18:15:54 simmim_pretrain] (main_simmim.py 184): INFO Train: [1/100][400/625]	eta 0:02:55 lr 0.000135	time 0.7729 (0.7781)	loss 2.2774 (2.3823)	grad_norm 12.5552 (14.3173)	mem 19489MB
[2022-04-02 18:17:11 simmim_pretrain] (main_simmim.py 184): INFO Train: [1/100][500/625]	eta 0:01:36 lr 0.000147	time 0.7721 (0.7756)	loss 2.2602 (2.3668)	grad_norm 11.7404 (14.2131)	mem 19489MB
[2022-04-02 18:18:28 simmim_pretrain] (main_simmim.py 184): INFO Train: [1/100][600/625]	eta 0:00:19 lr 0.000160	time 0.7669 (0.7740)	loss 2.2656 (2.3489)	grad_norm 18.9040 (14.0109)	mem 19489MB
[2022-04-02 18:18:47 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 1 training takes 0:08:04
[2022-04-02 18:18:50 simmim_pretrain] (main_simmim.py 184): INFO Train: [2/100][0/625]	eta 0:40:00 lr 0.000163	time 3.8414 (3.8414)	loss 2.2184 (2.2184)	grad_norm 13.8388 (13.8388)	mem 19489MB
[2022-04-02 18:20:07 simmim_pretrain] (main_simmim.py 184): INFO Train: [2/100][100/625]	eta 0:06:59 lr 0.000176	time 0.7670 (0.7993)	loss 2.1547 (2.2063)	grad_norm 12.8462 (12.7123)	mem 19489MB
[2022-04-02 18:21:24 simmim_pretrain] (main_simmim.py 184): INFO Train: [2/100][200/625]	eta 0:05:32 lr 0.000189	time 0.7818 (0.7833)	loss 2.1265 (2.1862)	grad_norm 10.5597 (12.3339)	mem 19489MB
[2022-04-02 18:22:41 simmim_pretrain] (main_simmim.py 184): INFO Train: [2/100][300/625]	eta 0:04:12 lr 0.000201	time 0.7713 (0.7781)	loss 2.0879 (2.1689)	grad_norm 8.8827 (12.0593)	mem 19489MB
[2022-04-02 18:23:58 simmim_pretrain] (main_simmim.py 184): INFO Train: [2/100][400/625]	eta 0:02:54 lr 0.000214	time 0.7663 (0.7755)	loss 2.1541 (2.1581)	grad_norm 12.4082 (12.0175)	mem 19489MB
[2022-04-02 18:25:14 simmim_pretrain] (main_simmim.py 184): INFO Train: [2/100][500/625]	eta 0:01:36 lr 0.000227	time 0.7543 (0.7740)	loss 2.0701 (2.1442)	grad_norm 11.3575 (11.8383)	mem 19489MB
[2022-04-02 18:26:31 simmim_pretrain] (main_simmim.py 184): INFO Train: [2/100][600/625]	eta 0:00:19 lr 0.000240	time 0.7723 (0.7734)	loss 2.0782 (2.1328)	grad_norm 11.9806 (11.7172)	mem 19489MB
[2022-04-02 18:26:50 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 2 training takes 0:08:03
[2022-04-02 18:26:54 simmim_pretrain] (main_simmim.py 184): INFO Train: [3/100][0/625]	eta 0:39:19 lr 0.000243	time 3.7758 (3.7758)	loss 2.0998 (2.0998)	grad_norm 11.7301 (11.7301)	mem 19489MB
[2022-04-02 18:28:11 simmim_pretrain] (main_simmim.py 184): INFO Train: [3/100][100/625]	eta 0:06:58 lr 0.000256	time 0.7762 (0.7966)	loss 2.0445 (2.0541)	grad_norm 9.7449 (10.6695)	mem 19489MB
[2022-04-02 18:29:27 simmim_pretrain] (main_simmim.py 184): INFO Train: [3/100][200/625]	eta 0:05:32 lr 0.000268	time 0.7683 (0.7821)	loss 2.0601 (2.0481)	grad_norm 9.6135 (10.8328)	mem 19489MB
[2022-04-02 18:30:44 simmim_pretrain] (main_simmim.py 184): INFO Train: [3/100][300/625]	eta 0:04:12 lr 0.000281	time 0.7682 (0.7771)	loss 2.0262 (2.0417)	grad_norm 11.2848 (10.7605)	mem 19489MB
[2022-04-02 18:32:01 simmim_pretrain] (main_simmim.py 184): INFO Train: [3/100][400/625]	eta 0:02:54 lr 0.000294	time 0.7658 (0.7747)	loss 2.0460 (2.0335)	grad_norm 9.9167 (10.5650)	mem 19489MB
[2022-04-02 18:33:18 simmim_pretrain] (main_simmim.py 184): INFO Train: [3/100][500/625]	eta 0:01:36 lr 0.000306	time 0.7709 (0.7735)	loss 1.9552 (2.0261)	grad_norm 7.8183 (10.5049)	mem 19489MB
[2022-04-02 18:34:34 simmim_pretrain] (main_simmim.py 184): INFO Train: [3/100][600/625]	eta 0:00:19 lr 0.000319	time 0.7643 (0.7725)	loss 2.0166 (2.0209)	grad_norm 8.9270 (10.4256)	mem 19489MB
[2022-04-02 18:34:53 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 3 training takes 0:08:03
[2022-04-02 18:34:57 simmim_pretrain] (main_simmim.py 184): INFO Train: [4/100][0/625]	eta 0:38:29 lr 0.000322	time 3.6957 (3.6957)	loss 1.9918 (1.9918)	grad_norm 8.9878 (8.9878)	mem 19489MB
[2022-04-02 18:36:14 simmim_pretrain] (main_simmim.py 184): INFO Train: [4/100][100/625]	eta 0:06:58 lr 0.000335	time 0.7713 (0.7972)	loss 1.9176 (1.9675)	grad_norm 7.3277 (9.7206)	mem 19489MB
[2022-04-02 18:37:31 simmim_pretrain] (main_simmim.py 184): INFO Train: [4/100][200/625]	eta 0:05:32 lr 0.000348	time 0.7694 (0.7824)	loss 1.9196 (1.9653)	grad_norm 7.4795 (9.9704)	mem 19489MB
[2022-04-02 18:38:47 simmim_pretrain] (main_simmim.py 184): INFO Train: [4/100][300/625]	eta 0:04:12 lr 0.000361	time 0.7774 (0.7773)	loss 1.9237 (1.9613)	grad_norm 9.0803 (9.8372)	mem 19489MB
[2022-04-02 18:40:04 simmim_pretrain] (main_simmim.py 184): INFO Train: [4/100][400/625]	eta 0:02:54 lr 0.000373	time 0.7582 (0.7756)	loss 1.8937 (1.9569)	grad_norm 10.3066 (9.8181)	mem 19489MB
[2022-04-02 18:41:21 simmim_pretrain] (main_simmim.py 184): INFO Train: [4/100][500/625]	eta 0:01:36 lr 0.000386	time 0.7735 (0.7738)	loss 1.9054 (1.9522)	grad_norm 13.1088 (9.7639)	mem 19489MB
[2022-04-02 18:42:37 simmim_pretrain] (main_simmim.py 184): INFO Train: [4/100][600/625]	eta 0:00:19 lr 0.000399	time 0.7589 (0.7724)	loss 1.9196 (1.9475)	grad_norm 9.0617 (9.7365)	mem 19489MB
[2022-04-02 18:42:56 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 4 training takes 0:08:02
[2022-04-02 18:43:00 simmim_pretrain] (main_simmim.py 184): INFO Train: [5/100][0/625]	eta 0:40:32 lr 0.000402	time 3.8921 (3.8921)	loss 1.9966 (1.9966)	grad_norm 11.8531 (11.8531)	mem 19489MB
[2022-04-02 18:44:17 simmim_pretrain] (main_simmim.py 184): INFO Train: [5/100][100/625]	eta 0:06:59 lr 0.000415	time 0.7703 (0.7984)	loss 1.9176 (1.9176)	grad_norm 11.2978 (9.2620)	mem 19489MB
[2022-04-02 18:45:33 simmim_pretrain] (main_simmim.py 184): INFO Train: [5/100][200/625]	eta 0:05:32 lr 0.000427	time 0.7628 (0.7825)	loss 1.8685 (1.9160)	grad_norm 9.0349 (9.5306)	mem 19489MB
[2022-04-02 18:46:50 simmim_pretrain] (main_simmim.py 184): INFO Train: [5/100][300/625]	eta 0:04:12 lr 0.000440	time 0.7712 (0.7774)	loss 1.8931 (1.9094)	grad_norm 12.2602 (9.4739)	mem 19490MB
[2022-04-02 18:48:07 simmim_pretrain] (main_simmim.py 184): INFO Train: [5/100][400/625]	eta 0:02:54 lr 0.000453	time 0.7676 (0.7746)	loss 1.8743 (1.9051)	grad_norm 7.7795 (9.5013)	mem 19490MB
[2022-04-02 18:49:23 simmim_pretrain] (main_simmim.py 184): INFO Train: [5/100][500/625]	eta 0:01:36 lr 0.000466	time 0.7626 (0.7731)	loss 1.8799 (1.9013)	grad_norm 9.9025 (9.5644)	mem 19490MB
[2022-04-02 18:50:40 simmim_pretrain] (main_simmim.py 184): INFO Train: [5/100][600/625]	eta 0:00:19 lr 0.000478	time 0.7695 (0.7720)	loss 1.8483 (1.8969)	grad_norm 8.0145 (9.4996)	mem 19490MB
[2022-04-02 18:50:59 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 5 training takes 0:08:02
[2022-04-02 18:51:04 simmim_pretrain] (main_simmim.py 184): INFO Train: [6/100][0/625]	eta 0:51:28 lr 0.000482	time 4.9420 (4.9420)	loss 1.8547 (1.8547)	grad_norm 9.4748 (9.4748)	mem 19490MB
[2022-04-02 18:52:21 simmim_pretrain] (main_simmim.py 184): INFO Train: [6/100][100/625]	eta 0:07:04 lr 0.000494	time 0.7580 (0.8091)	loss 1.8578 (1.8765)	grad_norm 6.6315 (9.2478)	mem 19490MB
[2022-04-02 18:53:38 simmim_pretrain] (main_simmim.py 184): INFO Train: [6/100][200/625]	eta 0:05:35 lr 0.000507	time 0.7681 (0.7901)	loss 1.8278 (1.8688)	grad_norm 11.4060 (9.0998)	mem 19490MB
[2022-04-02 18:54:55 simmim_pretrain] (main_simmim.py 184): INFO Train: [6/100][300/625]	eta 0:04:14 lr 0.000520	time 0.7533 (0.7828)	loss 1.8209 (1.8632)	grad_norm 7.8836 (8.9872)	mem 19490MB
[2022-04-02 18:56:11 simmim_pretrain] (main_simmim.py 184): INFO Train: [6/100][400/625]	eta 0:02:55 lr 0.000533	time 0.7676 (0.7790)	loss 1.8342 (1.8593)	grad_norm 7.9409 (8.9524)	mem 19490MB
[2022-04-02 18:57:28 simmim_pretrain] (main_simmim.py 184): INFO Train: [6/100][500/625]	eta 0:01:37 lr 0.000545	time 0.7666 (0.7767)	loss 1.8372 (1.8549)	grad_norm 9.1743 (8.9281)	mem 19490MB
[2022-04-02 18:58:45 simmim_pretrain] (main_simmim.py 184): INFO Train: [6/100][600/625]	eta 0:00:19 lr 0.000558	time 0.7766 (0.7752)	loss 1.8366 (1.8518)	grad_norm 8.4263 (8.9482)	mem 19490MB
[2022-04-02 18:59:04 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 6 training takes 0:08:04
[2022-04-02 18:59:07 simmim_pretrain] (main_simmim.py 184): INFO Train: [7/100][0/625]	eta 0:37:58 lr 0.000561	time 3.6462 (3.6462)	loss 1.8540 (1.8540)	grad_norm 7.4165 (7.4165)	mem 19490MB
[2022-04-02 19:00:24 simmim_pretrain] (main_simmim.py 184): INFO Train: [7/100][100/625]	eta 0:06:58 lr 0.000574	time 0.7652 (0.7969)	loss 1.8125 (1.8226)	grad_norm 7.4201 (8.7734)	mem 19490MB
[2022-04-02 19:01:41 simmim_pretrain] (main_simmim.py 184): INFO Train: [7/100][200/625]	eta 0:05:32 lr 0.000587	time 0.7607 (0.7817)	loss 1.8202 (1.8226)	grad_norm 11.4747 (8.7457)	mem 19490MB
[2022-04-02 19:02:58 simmim_pretrain] (main_simmim.py 184): INFO Train: [7/100][300/625]	eta 0:04:12 lr 0.000599	time 0.7697 (0.7773)	loss 1.8260 (1.8198)	grad_norm 8.3001 (8.6855)	mem 19490MB
[2022-04-02 19:04:14 simmim_pretrain] (main_simmim.py 184): INFO Train: [7/100][400/625]	eta 0:02:54 lr 0.000612	time 0.7711 (0.7748)	loss 1.7992 (1.8163)	grad_norm 8.3666 (8.6111)	mem 19490MB
[2022-04-02 19:05:31 simmim_pretrain] (main_simmim.py 184): INFO Train: [7/100][500/625]	eta 0:01:36 lr 0.000625	time 0.7679 (0.7734)	loss 1.8293 (1.8130)	grad_norm 7.8886 (8.5892)	mem 19490MB
[2022-04-02 19:06:48 simmim_pretrain] (main_simmim.py 184): INFO Train: [7/100][600/625]	eta 0:00:19 lr 0.000638	time 0.7602 (0.7731)	loss 1.8032 (1.8112)	grad_norm 8.2024 (8.5913)	mem 19490MB
[2022-04-02 19:07:07 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 7 training takes 0:08:03
[2022-04-02 19:07:11 simmim_pretrain] (main_simmim.py 184): INFO Train: [8/100][0/625]	eta 0:38:10 lr 0.000641	time 3.6640 (3.6640)	loss 1.7659 (1.7659)	grad_norm 6.1952 (6.1952)	mem 19490MB
[2022-04-02 19:08:28 simmim_pretrain] (main_simmim.py 184): INFO Train: [8/100][100/625]	eta 0:06:58 lr 0.000654	time 0.7696 (0.7977)	loss 1.7346 (1.7984)	grad_norm 9.1692 (8.4872)	mem 19490MB
[2022-04-02 19:09:44 simmim_pretrain] (main_simmim.py 184): INFO Train: [8/100][200/625]	eta 0:05:32 lr 0.000666	time 0.7673 (0.7822)	loss 1.7967 (1.7965)	grad_norm 9.3623 (8.2849)	mem 19490MB
[2022-04-02 19:11:01 simmim_pretrain] (main_simmim.py 184): INFO Train: [8/100][300/625]	eta 0:04:12 lr 0.000679	time 0.7652 (0.7774)	loss 1.8138 (1.7907)	grad_norm 6.8367 (8.0739)	mem 19490MB
[2022-04-02 19:12:18 simmim_pretrain] (main_simmim.py 184): INFO Train: [8/100][400/625]	eta 0:02:54 lr 0.000692	time 0.7657 (0.7748)	loss 1.7457 (1.7856)	grad_norm 10.0164 (8.0461)	mem 19490MB
[2022-04-02 19:13:35 simmim_pretrain] (main_simmim.py 184): INFO Train: [8/100][500/625]	eta 0:01:36 lr 0.000704	time 0.7674 (0.7733)	loss 1.7118 (1.7834)	grad_norm 5.3181 (8.1431)	mem 19490MB
[2022-04-02 19:14:51 simmim_pretrain] (main_simmim.py 184): INFO Train: [8/100][600/625]	eta 0:00:19 lr 0.000717	time 0.7640 (0.7726)	loss 1.7579 (1.7820)	grad_norm 9.2344 (8.2478)	mem 19490MB
[2022-04-02 19:15:10 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 8 training takes 0:08:03
[2022-04-02 19:15:14 simmim_pretrain] (main_simmim.py 184): INFO Train: [9/100][0/625]	eta 0:38:58 lr 0.000720	time 3.7415 (3.7415)	loss 1.7695 (1.7695)	grad_norm 8.0926 (8.0926)	mem 19490MB
[2022-04-02 19:16:31 simmim_pretrain] (main_simmim.py 184): INFO Train: [9/100][100/625]	eta 0:06:58 lr 0.000733	time 0.7719 (0.7971)	loss 1.7322 (1.7589)	grad_norm 6.4263 (7.5593)	mem 19490MB
[2022-04-02 19:17:47 simmim_pretrain] (main_simmim.py 184): INFO Train: [9/100][200/625]	eta 0:05:32 lr 0.000746	time 0.7629 (0.7823)	loss 1.8012 (1.7604)	grad_norm 10.6923 (7.8861)	mem 19490MB
[2022-04-02 19:19:04 simmim_pretrain] (main_simmim.py 184): INFO Train: [9/100][300/625]	eta 0:04:12 lr 0.000759	time 0.7706 (0.7772)	loss 1.7457 (1.7594)	grad_norm 9.3149 (7.8685)	mem 19490MB
[2022-04-02 19:20:21 simmim_pretrain] (main_simmim.py 184): INFO Train: [9/100][400/625]	eta 0:02:54 lr 0.000771	time 0.7599 (0.7755)	loss 1.7776 (1.7574)	grad_norm 10.8510 (7.7943)	mem 19490MB
[2022-04-02 19:21:38 simmim_pretrain] (main_simmim.py 184): INFO Train: [9/100][500/625]	eta 0:01:36 lr 0.000784	time 0.7544 (0.7739)	loss 1.7689 (1.7611)	grad_norm 9.9566 (inf)	mem 19490MB
[2022-04-02 19:22:55 simmim_pretrain] (main_simmim.py 184): INFO Train: [9/100][600/625]	eta 0:00:19 lr 0.000797	time 0.7581 (0.7729)	loss 1.6894 (1.7578)	grad_norm 6.0568 (inf)	mem 19490MB
[2022-04-02 19:23:13 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 9 training takes 0:08:03
[2022-04-02 19:23:17 simmim_pretrain] (main_simmim.py 184): INFO Train: [10/100][0/625]	eta 0:41:38 lr 0.000781	time 3.9977 (3.9977)	loss 1.7604 (1.7604)	grad_norm 8.5119 (8.5119)	mem 19490MB
[2022-04-02 19:24:34 simmim_pretrain] (main_simmim.py 184): INFO Train: [10/100][100/625]	eta 0:06:59 lr 0.000781	time 0.7720 (0.7994)	loss 1.7076 (1.7321)	grad_norm 6.3475 (6.7334)	mem 19490MB
[2022-04-02 19:25:51 simmim_pretrain] (main_simmim.py 184): INFO Train: [10/100][200/625]	eta 0:05:32 lr 0.000780	time 0.7691 (0.7835)	loss 1.7114 (1.7270)	grad_norm 4.2572 (6.7723)	mem 19490MB
[2022-04-02 19:27:08 simmim_pretrain] (main_simmim.py 184): INFO Train: [10/100][300/625]	eta 0:04:12 lr 0.000780	time 0.7666 (0.7780)	loss 1.7373 (1.7267)	grad_norm 6.6843 (6.7857)	mem 19490MB
[2022-04-02 19:28:24 simmim_pretrain] (main_simmim.py 184): INFO Train: [10/100][400/625]	eta 0:02:54 lr 0.000779	time 0.7654 (0.7751)	loss 1.7077 (1.7276)	grad_norm 6.6258 (6.8504)	mem 19490MB
[2022-04-02 19:29:41 simmim_pretrain] (main_simmim.py 184): INFO Train: [10/100][500/625]	eta 0:01:36 lr 0.000778	time 0.7611 (0.7731)	loss 1.6927 (1.7311)	grad_norm 6.3375 (6.9456)	mem 19490MB
[2022-04-02 19:30:57 simmim_pretrain] (main_simmim.py 184): INFO Train: [10/100][600/625]	eta 0:00:19 lr 0.000778	time 0.7663 (0.7718)	loss 1.7160 (1.7282)	grad_norm 6.9831 (6.8466)	mem 19490MB
[2022-04-02 19:31:16 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 10 training takes 0:08:02
[2022-04-02 19:31:21 simmim_pretrain] (main_simmim.py 184): INFO Train: [11/100][0/625]	eta 0:49:55 lr 0.000778	time 4.7924 (4.7924)	loss 1.7306 (1.7306)	grad_norm 6.7850 (6.7850)	mem 19490MB
[2022-04-02 19:32:38 simmim_pretrain] (main_simmim.py 184): INFO Train: [11/100][100/625]	eta 0:07:03 lr 0.000777	time 0.7640 (0.8068)	loss 1.7064 (1.7086)	grad_norm 5.7883 (6.0847)	mem 19490MB
[2022-04-02 19:33:55 simmim_pretrain] (main_simmim.py 184): INFO Train: [11/100][200/625]	eta 0:05:34 lr 0.000776	time 0.7745 (0.7881)	loss 1.7059 (1.7067)	grad_norm 4.0497 (6.0620)	mem 19490MB
[2022-04-02 19:35:11 simmim_pretrain] (main_simmim.py 184): INFO Train: [11/100][300/625]	eta 0:04:13 lr 0.000776	time 0.7694 (0.7813)	loss 1.7142 (1.7061)	grad_norm 4.9120 (6.0592)	mem 19490MB
[2022-04-02 19:36:28 simmim_pretrain] (main_simmim.py 184): INFO Train: [11/100][400/625]	eta 0:02:54 lr 0.000775	time 0.7651 (0.7775)	loss 1.6969 (1.7037)	grad_norm 4.9580 (5.9188)	mem 19490MB
[2022-04-02 19:37:45 simmim_pretrain] (main_simmim.py 184): INFO Train: [11/100][500/625]	eta 0:01:36 lr 0.000774	time 0.7692 (0.7755)	loss 1.6943 (1.7014)	grad_norm 5.9218 (5.9071)	mem 19490MB
[2022-04-02 19:39:01 simmim_pretrain] (main_simmim.py 184): INFO Train: [11/100][600/625]	eta 0:00:19 lr 0.000773	time 0.7577 (0.7741)	loss 1.6776 (1.6995)	grad_norm 3.6390 (5.8566)	mem 19490MB
[2022-04-02 19:39:20 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 11 training takes 0:08:04
[2022-04-02 19:39:24 simmim_pretrain] (main_simmim.py 184): INFO Train: [12/100][0/625]	eta 0:40:38 lr 0.000773	time 3.9009 (3.9009)	loss 1.6771 (1.6771)	grad_norm 4.0092 (4.0092)	mem 19490MB
[2022-04-02 19:40:41 simmim_pretrain] (main_simmim.py 184): INFO Train: [12/100][100/625]	eta 0:06:58 lr 0.000773	time 0.7609 (0.7974)	loss 1.6775 (1.6875)	grad_norm 5.1437 (5.3148)	mem 19490MB
[2022-04-02 19:41:58 simmim_pretrain] (main_simmim.py 184): INFO Train: [12/100][200/625]	eta 0:05:32 lr 0.000772	time 0.7661 (0.7820)	loss 1.6873 (1.6845)	grad_norm 4.7931 (5.2213)	mem 19490MB
[2022-04-02 19:43:14 simmim_pretrain] (main_simmim.py 184): INFO Train: [12/100][300/625]	eta 0:04:12 lr 0.000771	time 0.7576 (0.7771)	loss 1.6780 (1.6834)	grad_norm 5.6857 (5.2889)	mem 19490MB
[2022-04-02 19:44:31 simmim_pretrain] (main_simmim.py 184): INFO Train: [12/100][400/625]	eta 0:02:54 lr 0.000770	time 0.7647 (0.7746)	loss 1.6972 (1.6940)	grad_norm 5.7892 (5.5953)	mem 19490MB
[2022-04-02 19:45:48 simmim_pretrain] (main_simmim.py 184): INFO Train: [12/100][500/625]	eta 0:01:36 lr 0.000770	time 0.7582 (0.7730)	loss 1.6614 (1.6912)	grad_norm 3.7790 (5.3743)	mem 19490MB
[2022-04-02 19:47:05 simmim_pretrain] (main_simmim.py 184): INFO Train: [12/100][600/625]	eta 0:00:19 lr 0.000769	time 0.7746 (0.7722)	loss 1.6850 (1.6878)	grad_norm 5.4259 (5.2985)	mem 19490MB
[2022-04-02 19:47:23 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 12 training takes 0:08:03
[2022-04-02 19:47:29 simmim_pretrain] (main_simmim.py 184): INFO Train: [13/100][0/625]	eta 0:52:41 lr 0.000769	time 5.0586 (5.0586)	loss 1.6528 (1.6528)	grad_norm 5.3502 (5.3502)	mem 19490MB
[2022-04-02 19:48:45 simmim_pretrain] (main_simmim.py 184): INFO Train: [13/100][100/625]	eta 0:07:05 lr 0.000768	time 0.7869 (0.8111)	loss 1.6615 (1.6683)	grad_norm 3.3168 (4.7881)	mem 19490MB
[2022-04-02 19:50:02 simmim_pretrain] (main_simmim.py 184): INFO Train: [13/100][200/625]	eta 0:05:35 lr 0.000767	time 0.7698 (0.7895)	loss 1.6850 (1.6730)	grad_norm 5.3355 (4.8238)	mem 19490MB
[2022-04-02 19:51:19 simmim_pretrain] (main_simmim.py 184): INFO Train: [13/100][300/625]	eta 0:04:14 lr 0.000766	time 0.7612 (0.7827)	loss 1.6438 (1.6695)	grad_norm 3.8769 (4.6542)	mem 19490MB
[2022-04-02 19:52:36 simmim_pretrain] (main_simmim.py 184): INFO Train: [13/100][400/625]	eta 0:02:55 lr 0.000766	time 0.7645 (0.7791)	loss 1.6728 (1.6671)	grad_norm 5.0746 (4.6613)	mem 19490MB
[2022-04-02 19:53:53 simmim_pretrain] (main_simmim.py 184): INFO Train: [13/100][500/625]	eta 0:01:37 lr 0.000765	time 0.7684 (0.7771)	loss 1.6720 (1.6655)	grad_norm 4.2688 (4.5474)	mem 19490MB
[2022-04-02 19:55:10 simmim_pretrain] (main_simmim.py 184): INFO Train: [13/100][600/625]	eta 0:00:19 lr 0.000764	time 0.7661 (0.7755)	loss 1.6436 (1.6642)	grad_norm 3.6895 (4.4897)	mem 19490MB
[2022-04-02 19:55:28 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 13 training takes 0:08:04
[2022-04-02 19:55:32 simmim_pretrain] (main_simmim.py 184): INFO Train: [14/100][0/625]	eta 0:41:59 lr 0.000764	time 4.0308 (4.0308)	loss 1.6425 (1.6425)	grad_norm 3.3079 (3.3079)	mem 19490MB
[2022-04-02 19:56:49 simmim_pretrain] (main_simmim.py 184): INFO Train: [14/100][100/625]	eta 0:06:59 lr 0.000763	time 0.7720 (0.7998)	loss 1.6545 (1.6541)	grad_norm 4.5363 (4.3131)	mem 19490MB
[2022-04-02 19:58:06 simmim_pretrain] (main_simmim.py 184): INFO Train: [14/100][200/625]	eta 0:05:33 lr 0.000762	time 0.7704 (0.7836)	loss 1.6354 (1.6515)	grad_norm 4.5959 (4.3190)	mem 19490MB
[2022-04-02 19:59:23 simmim_pretrain] (main_simmim.py 184): INFO Train: [14/100][300/625]	eta 0:04:13 lr 0.000761	time 0.7695 (0.7785)	loss 1.6224 (1.6505)	grad_norm 3.0403 (4.1619)	mem 19490MB
[2022-04-02 20:00:39 simmim_pretrain] (main_simmim.py 184): INFO Train: [14/100][400/625]	eta 0:02:54 lr 0.000761	time 0.7700 (0.7757)	loss 1.6473 (1.6565)	grad_norm 2.1599 (inf)	mem 19490MB
[2022-04-02 20:01:56 simmim_pretrain] (main_simmim.py 184): INFO Train: [14/100][500/625]	eta 0:01:36 lr 0.000760	time 0.7646 (0.7745)	loss 1.6463 (1.6551)	grad_norm 2.2927 (inf)	mem 19490MB
[2022-04-02 20:03:13 simmim_pretrain] (main_simmim.py 184): INFO Train: [14/100][600/625]	eta 0:00:19 lr 0.000759	time 0.7638 (0.7734)	loss 1.6291 (1.6533)	grad_norm 2.6248 (inf)	mem 19490MB
[2022-04-02 20:03:32 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 14 training takes 0:08:03
[2022-04-02 20:03:36 simmim_pretrain] (main_simmim.py 184): INFO Train: [15/100][0/625]	eta 0:38:11 lr 0.000759	time 3.6656 (3.6656)	loss 1.6255 (1.6255)	grad_norm 4.5233 (4.5233)	mem 19490MB
[2022-04-02 20:04:52 simmim_pretrain] (main_simmim.py 184): INFO Train: [15/100][100/625]	eta 0:06:58 lr 0.000758	time 0.7710 (0.7964)	loss 1.6758 (1.6474)	grad_norm 5.2122 (4.2949)	mem 19490MB
[2022-04-02 20:06:09 simmim_pretrain] (main_simmim.py 184): INFO Train: [15/100][200/625]	eta 0:05:32 lr 0.000757	time 0.7699 (0.7814)	loss 1.6247 (1.6425)	grad_norm 2.3717 (4.0975)	mem 19490MB
[2022-04-02 20:07:26 simmim_pretrain] (main_simmim.py 184): INFO Train: [15/100][300/625]	eta 0:04:12 lr 0.000756	time 0.7628 (0.7767)	loss 1.6266 (1.6422)	grad_norm 4.2622 (4.0609)	mem 19490MB
[2022-04-02 20:08:42 simmim_pretrain] (main_simmim.py 184): INFO Train: [15/100][400/625]	eta 0:02:54 lr 0.000755	time 0.7604 (0.7739)	loss 1.6418 (1.6404)	grad_norm 4.2762 (3.9506)	mem 19490MB
[2022-04-02 20:09:59 simmim_pretrain] (main_simmim.py 184): INFO Train: [15/100][500/625]	eta 0:01:36 lr 0.000754	time 0.7728 (0.7726)	loss 1.6392 (1.6397)	grad_norm 3.5791 (3.8738)	mem 19490MB
[2022-04-02 20:11:16 simmim_pretrain] (main_simmim.py 184): INFO Train: [15/100][600/625]	eta 0:00:19 lr 0.000753	time 0.7676 (0.7716)	loss 1.6236 (1.6388)	grad_norm 4.0664 (3.9094)	mem 19490MB
[2022-04-02 20:11:35 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 15 training takes 0:08:02
[2022-04-02 20:11:40 simmim_pretrain] (main_simmim.py 184): INFO Train: [16/100][0/625]	eta 0:52:06 lr 0.000753	time 5.0029 (5.0029)	loss 1.6131 (1.6131)	grad_norm 3.1327 (3.1327)	mem 19490MB
[2022-04-02 20:12:56 simmim_pretrain] (main_simmim.py 184): INFO Train: [16/100][100/625]	eta 0:07:05 lr 0.000752	time 0.7631 (0.8097)	loss 1.6918 (1.6660)	grad_norm 6.7078 (5.1350)	mem 19490MB
[2022-04-02 20:14:13 simmim_pretrain] (main_simmim.py 184): INFO Train: [16/100][200/625]	eta 0:05:35 lr 0.000751	time 0.7707 (0.7896)	loss 1.6280 (1.6542)	grad_norm 3.1941 (4.4946)	mem 19490MB
[2022-04-02 20:15:30 simmim_pretrain] (main_simmim.py 184): INFO Train: [16/100][300/625]	eta 0:04:14 lr 0.000750	time 0.7678 (0.7832)	loss 1.6327 (1.6464)	grad_norm 3.6076 (4.1930)	mem 19490MB
[2022-04-02 20:16:47 simmim_pretrain] (main_simmim.py 184): INFO Train: [16/100][400/625]	eta 0:02:55 lr 0.000749	time 0.7653 (0.7791)	loss 1.6151 (1.6419)	grad_norm 4.4805 (4.0003)	mem 19490MB
[2022-04-02 20:18:04 simmim_pretrain] (main_simmim.py 184): INFO Train: [16/100][500/625]	eta 0:01:37 lr 0.000748	time 0.7707 (0.7766)	loss 1.6531 (1.6393)	grad_norm 5.0419 (3.9108)	mem 19490MB
[2022-04-02 20:19:20 simmim_pretrain] (main_simmim.py 184): INFO Train: [16/100][600/625]	eta 0:00:19 lr 0.000747	time 0.7620 (0.7750)	loss 1.6135 (1.6373)	grad_norm 3.4873 (3.8442)	mem 19490MB
[2022-04-02 20:19:39 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 16 training takes 0:08:04
[2022-04-02 20:19:43 simmim_pretrain] (main_simmim.py 184): INFO Train: [17/100][0/625]	eta 0:38:34 lr 0.000747	time 3.7029 (3.7029)	loss 1.6071 (1.6071)	grad_norm 4.1878 (4.1878)	mem 19490MB
[2022-04-02 20:21:00 simmim_pretrain] (main_simmim.py 184): INFO Train: [17/100][100/625]	eta 0:06:58 lr 0.000746	time 0.7675 (0.7969)	loss 1.6125 (1.6241)	grad_norm 2.4204 (3.6682)	mem 19490MB
[2022-04-02 20:22:16 simmim_pretrain] (main_simmim.py 184): INFO Train: [17/100][200/625]	eta 0:05:32 lr 0.000745	time 0.7638 (0.7822)	loss 1.6318 (1.6220)	grad_norm 3.6663 (3.4688)	mem 19490MB
[2022-04-02 20:23:33 simmim_pretrain] (main_simmim.py 184): INFO Train: [17/100][300/625]	eta 0:04:12 lr 0.000744	time 0.7704 (0.7776)	loss 1.6267 (1.6212)	grad_norm 2.3295 (3.4404)	mem 19490MB
[2022-04-02 20:24:50 simmim_pretrain] (main_simmim.py 184): INFO Train: [17/100][400/625]	eta 0:02:54 lr 0.000743	time 0.7662 (0.7750)	loss 1.6191 (1.6209)	grad_norm 2.3859 (3.4486)	mem 19490MB
[2022-04-02 20:26:07 simmim_pretrain] (main_simmim.py 184): INFO Train: [17/100][500/625]	eta 0:01:36 lr 0.000742	time 0.7650 (0.7733)	loss 1.5977 (1.6203)	grad_norm 3.6131 (3.4689)	mem 19490MB
[2022-04-02 20:27:23 simmim_pretrain] (main_simmim.py 184): INFO Train: [17/100][600/625]	eta 0:00:19 lr 0.000741	time 0.7766 (0.7724)	loss 1.6213 (1.6204)	grad_norm 3.7832 (3.4154)	mem 19490MB
[2022-04-02 20:27:42 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 17 training takes 0:08:02
[2022-04-02 20:27:46 simmim_pretrain] (main_simmim.py 184): INFO Train: [18/100][0/625]	eta 0:43:36 lr 0.000741	time 4.1861 (4.1861)	loss 1.6388 (1.6388)	grad_norm 2.3857 (2.3857)	mem 19490MB
[2022-04-02 20:29:03 simmim_pretrain] (main_simmim.py 184): INFO Train: [18/100][100/625]	eta 0:07:02 lr 0.000740	time 0.7644 (0.8057)	loss 1.6386 (1.6145)	grad_norm 4.1052 (3.2152)	mem 19490MB
[2022-04-02 20:30:20 simmim_pretrain] (main_simmim.py 184): INFO Train: [18/100][200/625]	eta 0:05:34 lr 0.000739	time 0.7654 (0.7862)	loss 1.6325 (1.6519)	grad_norm 2.5045 (inf)	mem 19490MB
[2022-04-02 20:31:37 simmim_pretrain] (main_simmim.py 184): INFO Train: [18/100][300/625]	eta 0:04:13 lr 0.000738	time 0.7611 (0.7799)	loss 1.6292 (1.6409)	grad_norm 3.7942 (inf)	mem 19490MB
[2022-04-02 20:32:53 simmim_pretrain] (main_simmim.py 184): INFO Train: [18/100][400/625]	eta 0:02:54 lr 0.000737	time 0.7742 (0.7765)	loss 1.6054 (1.6342)	grad_norm 2.3318 (inf)	mem 19490MB
[2022-04-02 20:34:10 simmim_pretrain] (main_simmim.py 184): INFO Train: [18/100][500/625]	eta 0:01:36 lr 0.000736	time 0.7677 (0.7746)	loss 1.6290 (1.6295)	grad_norm 2.4710 (inf)	mem 19490MB
[2022-04-02 20:35:27 simmim_pretrain] (main_simmim.py 184): INFO Train: [18/100][600/625]	eta 0:00:19 lr 0.000735	time 0.7733 (0.7733)	loss 1.6028 (1.6264)	grad_norm 3.2992 (inf)	mem 19490MB
[2022-04-02 20:35:46 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 18 training takes 0:08:03
[2022-04-02 20:35:50 simmim_pretrain] (main_simmim.py 184): INFO Train: [19/100][0/625]	eta 0:42:20 lr 0.000734	time 4.0643 (4.0643)	loss 1.5924 (1.5924)	grad_norm 1.8727 (1.8727)	mem 19490MB
[2022-04-02 20:37:07 simmim_pretrain] (main_simmim.py 184): INFO Train: [19/100][100/625]	eta 0:07:00 lr 0.000733	time 0.7744 (0.8007)	loss 1.6100 (1.6118)	grad_norm 2.7289 (2.8855)	mem 19490MB
[2022-04-02 20:38:23 simmim_pretrain] (main_simmim.py 184): INFO Train: [19/100][200/625]	eta 0:05:33 lr 0.000732	time 0.7680 (0.7837)	loss 1.6067 (1.6088)	grad_norm 2.4970 (3.0336)	mem 19490MB
[2022-04-02 20:39:40 simmim_pretrain] (main_simmim.py 184): INFO Train: [19/100][300/625]	eta 0:04:12 lr 0.000731	time 0.7695 (0.7784)	loss 1.5835 (1.6079)	grad_norm 2.0998 (3.1058)	mem 19490MB
[2022-04-02 20:40:57 simmim_pretrain] (main_simmim.py 184): INFO Train: [19/100][400/625]	eta 0:02:54 lr 0.000730	time 0.7653 (0.7756)	loss 1.5689 (1.6082)	grad_norm 3.4456 (3.0993)	mem 19490MB
[2022-04-02 20:42:14 simmim_pretrain] (main_simmim.py 184): INFO Train: [19/100][500/625]	eta 0:01:36 lr 0.000729	time 0.7635 (0.7747)	loss 1.6256 (1.6078)	grad_norm 2.8663 (3.0824)	mem 19490MB
[2022-04-02 20:43:31 simmim_pretrain] (main_simmim.py 184): INFO Train: [19/100][600/625]	eta 0:00:19 lr 0.000728	time 0.7675 (0.7735)	loss 1.6185 (1.6072)	grad_norm 1.7704 (3.0770)	mem 19490MB
[2022-04-02 20:43:49 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 19 training takes 0:08:03
[2022-04-02 20:43:53 simmim_pretrain] (main_simmim.py 184): INFO Train: [20/100][0/625]	eta 0:41:41 lr 0.000727	time 4.0025 (4.0025)	loss 1.6416 (1.6416)	grad_norm 2.7165 (2.7165)	mem 19490MB
[2022-04-02 20:45:10 simmim_pretrain] (main_simmim.py 184): INFO Train: [20/100][100/625]	eta 0:07:00 lr 0.000726	time 0.7678 (0.8005)	loss 1.5948 (1.6250)	grad_norm 3.7453 (4.0784)	mem 19490MB
[2022-04-02 20:46:27 simmim_pretrain] (main_simmim.py 184): INFO Train: [20/100][200/625]	eta 0:05:33 lr 0.000725	time 0.7630 (0.7836)	loss 1.6050 (1.6128)	grad_norm 2.9600 (3.4159)	mem 19490MB
[2022-04-02 20:47:44 simmim_pretrain] (main_simmim.py 184): INFO Train: [20/100][300/625]	eta 0:04:12 lr 0.000724	time 0.7684 (0.7782)	loss 1.5934 (1.6086)	grad_norm 2.0827 (3.2132)	mem 19490MB
[2022-04-02 20:49:00 simmim_pretrain] (main_simmim.py 184): INFO Train: [20/100][400/625]	eta 0:02:54 lr 0.000723	time 0.7701 (0.7753)	loss 1.5985 (1.6075)	grad_norm 2.7974 (3.1008)	mem 19490MB
[2022-04-02 20:50:17 simmim_pretrain] (main_simmim.py 184): INFO Train: [20/100][500/625]	eta 0:01:36 lr 0.000722	time 0.7667 (0.7737)	loss 1.6092 (1.6098)	grad_norm 1.6313 (3.2769)	mem 19490MB
[2022-04-02 20:51:34 simmim_pretrain] (main_simmim.py 184): INFO Train: [20/100][600/625]	eta 0:00:19 lr 0.000721	time 0.7658 (0.7726)	loss 1.5778 (1.6079)	grad_norm 1.8675 (3.1786)	mem 19490MB
[2022-04-02 20:51:53 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 20 training takes 0:08:03
[2022-04-02 20:51:57 simmim_pretrain] (main_simmim.py 184): INFO Train: [21/100][0/625]	eta 0:49:10 lr 0.000720	time 4.7209 (4.7209)	loss 1.6473 (1.6473)	grad_norm 4.2746 (4.2746)	mem 19490MB
[2022-04-02 20:53:14 simmim_pretrain] (main_simmim.py 184): INFO Train: [21/100][100/625]	eta 0:07:03 lr 0.000719	time 0.7568 (0.8070)	loss 1.6134 (1.6076)	grad_norm 1.8154 (3.1119)	mem 19490MB
[2022-04-02 20:54:31 simmim_pretrain] (main_simmim.py 184): INFO Train: [21/100][200/625]	eta 0:05:34 lr 0.000718	time 0.7714 (0.7870)	loss 1.5910 (1.6037)	grad_norm 2.2370 (3.0320)	mem 19490MB
[2022-04-02 20:55:48 simmim_pretrain] (main_simmim.py 184): INFO Train: [21/100][300/625]	eta 0:04:14 lr 0.000717	time 0.7636 (0.7825)	loss 1.6111 (1.6008)	grad_norm 2.8784 (2.9159)	mem 19490MB
[2022-04-02 20:57:05 simmim_pretrain] (main_simmim.py 184): INFO Train: [21/100][400/625]	eta 0:02:55 lr 0.000716	time 0.7662 (0.7785)	loss 1.5932 (1.5999)	grad_norm 2.0392 (2.8564)	mem 19490MB
[2022-04-02 20:58:21 simmim_pretrain] (main_simmim.py 184): INFO Train: [21/100][500/625]	eta 0:01:37 lr 0.000714	time 0.7621 (0.7761)	loss 1.6093 (1.5991)	grad_norm 2.9754 (2.8062)	mem 19490MB
[2022-04-02 20:59:38 simmim_pretrain] (main_simmim.py 184): INFO Train: [21/100][600/625]	eta 0:00:19 lr 0.000713	time 0.7689 (0.7746)	loss 1.6410 (1.5990)	grad_norm 2.5499 (2.8438)	mem 19490MB
[2022-04-02 20:59:57 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 21 training takes 0:08:04
[2022-04-02 21:00:01 simmim_pretrain] (main_simmim.py 184): INFO Train: [22/100][0/625]	eta 0:40:57 lr 0.000713	time 3.9326 (3.9326)	loss 1.5768 (1.5768)	grad_norm 2.6157 (2.6157)	mem 19490MB
[2022-04-02 21:01:18 simmim_pretrain] (main_simmim.py 184): INFO Train: [22/100][100/625]	eta 0:06:59 lr 0.000712	time 0.7841 (0.7994)	loss 1.6021 (1.5934)	grad_norm 2.4820 (2.5010)	mem 19490MB
[2022-04-02 21:02:35 simmim_pretrain] (main_simmim.py 184): INFO Train: [22/100][200/625]	eta 0:05:33 lr 0.000710	time 0.7651 (0.7838)	loss 1.5709 (1.5928)	grad_norm 3.1822 (2.6138)	mem 19490MB
[2022-04-02 21:03:51 simmim_pretrain] (main_simmim.py 184): INFO Train: [22/100][300/625]	eta 0:04:13 lr 0.000709	time 0.7695 (0.7787)	loss 1.6078 (1.5935)	grad_norm 3.0508 (2.6395)	mem 19490MB
[2022-04-02 21:05:08 simmim_pretrain] (main_simmim.py 184): INFO Train: [22/100][400/625]	eta 0:02:54 lr 0.000708	time 0.7658 (0.7759)	loss 1.5591 (1.5926)	grad_norm 2.0427 (2.6019)	mem 19490MB
[2022-04-02 21:06:25 simmim_pretrain] (main_simmim.py 184): INFO Train: [22/100][500/625]	eta 0:01:36 lr 0.000707	time 0.7689 (0.7743)	loss 1.5991 (1.5928)	grad_norm 2.7983 (2.6680)	mem 19490MB
[2022-04-02 21:07:42 simmim_pretrain] (main_simmim.py 184): INFO Train: [22/100][600/625]	eta 0:00:19 lr 0.000705	time 0.7759 (0.7733)	loss 1.5638 (1.5920)	grad_norm 2.8421 (2.6697)	mem 19490MB
[2022-04-02 21:08:01 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 22 training takes 0:08:03
[2022-04-02 21:08:05 simmim_pretrain] (main_simmim.py 184): INFO Train: [23/100][0/625]	eta 0:44:02 lr 0.000705	time 4.2274 (4.2274)	loss 1.6979 (1.6979)	grad_norm 9.9446 (9.9446)	mem 19490MB
[2022-04-02 21:09:22 simmim_pretrain] (main_simmim.py 184): INFO Train: [23/100][100/625]	eta 0:07:03 lr 0.000704	time 0.7603 (0.8068)	loss 1.5640 (1.6170)	grad_norm 2.6881 (3.3910)	mem 19490MB
[2022-04-02 21:10:39 simmim_pretrain] (main_simmim.py 184): INFO Train: [23/100][200/625]	eta 0:05:34 lr 0.000703	time 0.7587 (0.7870)	loss 1.5703 (1.6034)	grad_norm 2.8639 (3.0177)	mem 19490MB
[2022-04-02 21:11:55 simmim_pretrain] (main_simmim.py 184): INFO Train: [23/100][300/625]	eta 0:04:13 lr 0.000701	time 0.7778 (0.7805)	loss 1.5830 (1.5993)	grad_norm 2.9876 (2.7679)	mem 19490MB
[2022-04-02 21:13:12 simmim_pretrain] (main_simmim.py 184): INFO Train: [23/100][400/625]	eta 0:02:54 lr 0.000700	time 0.7592 (0.7768)	loss 1.5822 (1.5960)	grad_norm 2.1706 (2.6816)	mem 19490MB
[2022-04-02 21:14:29 simmim_pretrain] (main_simmim.py 184): INFO Train: [23/100][500/625]	eta 0:01:36 lr 0.000699	time 0.7652 (0.7746)	loss 1.6095 (1.5944)	grad_norm 1.7614 (2.6889)	mem 19490MB
[2022-04-02 21:15:45 simmim_pretrain] (main_simmim.py 184): INFO Train: [23/100][600/625]	eta 0:00:19 lr 0.000697	time 0.7789 (0.7735)	loss 1.5817 (1.5932)	grad_norm 2.9579 (2.6900)	mem 19490MB
[2022-04-02 21:16:04 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 23 training takes 0:08:03
[2022-04-02 21:16:08 simmim_pretrain] (main_simmim.py 184): INFO Train: [24/100][0/625]	eta 0:40:31 lr 0.000697	time 3.8905 (3.8905)	loss 1.5477 (1.5477)	grad_norm 2.3017 (2.3017)	mem 19490MB
[2022-04-02 21:17:25 simmim_pretrain] (main_simmim.py 184): INFO Train: [24/100][100/625]	eta 0:07:00 lr 0.000696	time 0.7749 (0.8001)	loss 1.5946 (1.5834)	grad_norm 3.9139 (2.4323)	mem 19490MB
[2022-04-02 21:18:42 simmim_pretrain] (main_simmim.py 184): INFO Train: [24/100][200/625]	eta 0:05:32 lr 0.000694	time 0.7709 (0.7835)	loss 1.6196 (1.5984)	grad_norm 2.5209 (3.3579)	mem 19490MB
[2022-04-02 21:19:58 simmim_pretrain] (main_simmim.py 184): INFO Train: [24/100][300/625]	eta 0:04:12 lr 0.000693	time 0.7701 (0.7782)	loss 1.5861 (1.5950)	grad_norm 1.7813 (2.9392)	mem 19490MB
[2022-04-02 21:21:15 simmim_pretrain] (main_simmim.py 184): INFO Train: [24/100][400/625]	eta 0:02:54 lr 0.000692	time 0.7575 (0.7758)	loss 1.6259 (1.5919)	grad_norm 1.7908 (2.7433)	mem 19490MB
[2022-04-02 21:22:32 simmim_pretrain] (main_simmim.py 184): INFO Train: [24/100][500/625]	eta 0:01:36 lr 0.000690	time 0.8385 (0.7748)	loss 1.5884 (1.5902)	grad_norm 2.8665 (2.6492)	mem 19490MB
[2022-04-02 21:23:49 simmim_pretrain] (main_simmim.py 184): INFO Train: [24/100][600/625]	eta 0:00:19 lr 0.000689	time 0.7636 (0.7737)	loss 1.5685 (1.5888)	grad_norm 2.6165 (2.6153)	mem 19490MB
[2022-04-02 21:24:08 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 24 training takes 0:08:03
[2022-04-02 21:24:12 simmim_pretrain] (main_simmim.py 184): INFO Train: [25/100][0/625]	eta 0:39:33 lr 0.000689	time 3.7972 (3.7972)	loss 1.5920 (1.5920)	grad_norm 2.8557 (2.8557)	mem 19490MB
[2022-04-02 21:25:29 simmim_pretrain] (main_simmim.py 184): INFO Train: [25/100][100/625]	eta 0:06:58 lr 0.000687	time 0.7767 (0.7977)	loss 1.5810 (1.5849)	grad_norm 2.6737 (2.5266)	mem 19490MB
[2022-04-02 21:26:45 simmim_pretrain] (main_simmim.py 184): INFO Train: [25/100][200/625]	eta 0:05:32 lr 0.000686	time 0.7799 (0.7826)	loss 1.5868 (1.5830)	grad_norm 2.5114 (2.3631)	mem 19490MB
[2022-04-02 21:28:02 simmim_pretrain] (main_simmim.py 184): INFO Train: [25/100][300/625]	eta 0:04:12 lr 0.000685	time 0.7739 (0.7773)	loss 1.5808 (1.5811)	grad_norm 1.7745 (2.2797)	mem 19490MB
[2022-04-02 21:29:19 simmim_pretrain] (main_simmim.py 184): INFO Train: [25/100][400/625]	eta 0:02:54 lr 0.000683	time 0.7698 (0.7747)	loss 1.5847 (1.5809)	grad_norm 2.9656 (2.3450)	mem 19490MB
[2022-04-02 21:30:35 simmim_pretrain] (main_simmim.py 184): INFO Train: [25/100][500/625]	eta 0:01:36 lr 0.000682	time 0.7616 (0.7729)	loss 1.5707 (1.5803)	grad_norm 2.2876 (2.2965)	mem 19490MB
[2022-04-02 21:31:52 simmim_pretrain] (main_simmim.py 184): INFO Train: [25/100][600/625]	eta 0:00:19 lr 0.000680	time 0.7693 (0.7721)	loss 1.5922 (1.5800)	grad_norm 2.2876 (2.3108)	mem 19490MB
[2022-04-02 21:32:11 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 25 training takes 0:08:02
[2022-04-02 21:32:16 simmim_pretrain] (main_simmim.py 184): INFO Train: [26/100][0/625]	eta 0:52:14 lr 0.000680	time 5.0152 (5.0152)	loss 1.5464 (1.5464)	grad_norm 2.3431 (2.3431)	mem 19490MB
[2022-04-02 21:33:33 simmim_pretrain] (main_simmim.py 184): INFO Train: [26/100][100/625]	eta 0:07:05 lr 0.000679	time 0.7664 (0.8107)	loss 1.5752 (1.5779)	grad_norm 2.7379 (2.1646)	mem 19490MB
[2022-04-02 21:34:50 simmim_pretrain] (main_simmim.py 184): INFO Train: [26/100][200/625]	eta 0:05:35 lr 0.000677	time 0.7765 (0.7890)	loss 6.1755 (1.8532)	grad_norm 69.2424 (inf)	mem 19490MB
[2022-04-02 21:36:07 simmim_pretrain] (main_simmim.py 184): INFO Train: [26/100][300/625]	eta 0:04:14 lr 0.000676	time 0.7562 (0.7827)	loss 2.6504 (2.6685)	grad_norm 2.8200 (inf)	mem 19490MB
[2022-04-02 21:37:23 simmim_pretrain] (main_simmim.py 184): INFO Train: [26/100][400/625]	eta 0:02:55 lr 0.000675	time 0.7676 (0.7785)	loss 2.4823 (2.6491)	grad_norm 3.3432 (inf)	mem 19490MB
[2022-04-02 21:38:40 simmim_pretrain] (main_simmim.py 184): INFO Train: [26/100][500/625]	eta 0:01:36 lr 0.000673	time 0.7446 (0.7759)	loss 2.3870 (2.6076)	grad_norm 2.8164 (inf)	mem 19490MB
[2022-04-02 21:39:56 simmim_pretrain] (main_simmim.py 184): INFO Train: [26/100][600/625]	eta 0:00:19 lr 0.000672	time 0.7752 (0.7745)	loss 2.3464 (2.5656)	grad_norm 3.5312 (inf)	mem 19490MB
[2022-04-02 21:40:15 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 26 training takes 0:08:04
[2022-04-02 21:40:19 simmim_pretrain] (main_simmim.py 184): INFO Train: [27/100][0/625]	eta 0:39:52 lr 0.000671	time 3.8273 (3.8273)	loss 2.3717 (2.3717)	grad_norm 4.2529 (4.2529)	mem 19490MB
[2022-04-02 21:41:36 simmim_pretrain] (main_simmim.py 184): INFO Train: [27/100][100/625]	eta 0:06:57 lr 0.000670	time 0.7646 (0.7962)	loss 2.1339 (2.2520)	grad_norm 4.1768 (4.4452)	mem 19490MB
[2022-04-02 21:42:52 simmim_pretrain] (main_simmim.py 184): INFO Train: [27/100][200/625]	eta 0:05:31 lr 0.000668	time 0.7528 (0.7804)	loss 2.0671 (2.2035)	grad_norm 2.7399 (4.0014)	mem 19490MB
[2022-04-02 21:44:09 simmim_pretrain] (main_simmim.py 184): INFO Train: [27/100][300/625]	eta 0:04:12 lr 0.000667	time 0.7599 (0.7754)	loss 2.0764 (2.1689)	grad_norm 2.9102 (3.8875)	mem 19490MB
[2022-04-02 21:45:25 simmim_pretrain] (main_simmim.py 184): INFO Train: [27/100][400/625]	eta 0:02:53 lr 0.000666	time 0.7616 (0.7730)	loss 2.0730 (2.1432)	grad_norm 4.6974 (4.0231)	mem 19490MB
[2022-04-02 21:46:42 simmim_pretrain] (main_simmim.py 184): INFO Train: [27/100][500/625]	eta 0:01:36 lr 0.000664	time 0.7574 (0.7716)	loss 1.9726 (2.1150)	grad_norm 2.7586 (4.0157)	mem 19490MB
[2022-04-02 21:47:58 simmim_pretrain] (main_simmim.py 184): INFO Train: [27/100][600/625]	eta 0:00:19 lr 0.000663	time 0.7733 (0.7709)	loss 1.9528 (2.0885)	grad_norm 2.4670 (3.9927)	mem 19490MB
[2022-04-02 21:48:17 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 27 training takes 0:08:02
[2022-04-02 21:48:21 simmim_pretrain] (main_simmim.py 184): INFO Train: [28/100][0/625]	eta 0:39:48 lr 0.000662	time 3.8219 (3.8219)	loss 1.9842 (1.9842)	grad_norm 4.1102 (4.1102)	mem 19490MB
[2022-04-02 21:49:38 simmim_pretrain] (main_simmim.py 184): INFO Train: [28/100][100/625]	eta 0:07:00 lr 0.000661	time 0.7578 (0.8002)	loss 1.9234 (1.9248)	grad_norm 4.5045 (3.9816)	mem 19490MB
[2022-04-02 21:50:55 simmim_pretrain] (main_simmim.py 184): INFO Train: [28/100][200/625]	eta 0:05:33 lr 0.000659	time 0.7728 (0.7841)	loss 1.8763 (1.9095)	grad_norm 2.9221 (3.8282)	mem 19490MB
[2022-04-02 21:52:12 simmim_pretrain] (main_simmim.py 184): INFO Train: [28/100][300/625]	eta 0:04:13 lr 0.000658	time 0.7635 (0.7785)	loss 1.8630 (1.8990)	grad_norm 1.5070 (3.7654)	mem 19490MB
[2022-04-02 21:53:28 simmim_pretrain] (main_simmim.py 184): INFO Train: [28/100][400/625]	eta 0:02:54 lr 0.000656	time 0.7669 (0.7757)	loss 1.8238 (1.8868)	grad_norm 3.5816 (3.6399)	mem 19490MB
[2022-04-02 21:54:45 simmim_pretrain] (main_simmim.py 184): INFO Train: [28/100][500/625]	eta 0:01:36 lr 0.000655	time 0.7686 (0.7740)	loss 1.8249 (1.8746)	grad_norm 4.8590 (3.5429)	mem 19490MB
[2022-04-02 21:56:02 simmim_pretrain] (main_simmim.py 184): INFO Train: [28/100][600/625]	eta 0:00:19 lr 0.000653	time 0.7711 (0.7728)	loss 1.7908 (1.8635)	grad_norm 3.2400 (3.5265)	mem 19490MB
[2022-04-02 21:56:21 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 28 training takes 0:08:03
[2022-04-02 21:56:24 simmim_pretrain] (main_simmim.py 184): INFO Train: [29/100][0/625]	eta 0:38:55 lr 0.000653	time 3.7361 (3.7361)	loss 1.7608 (1.7608)	grad_norm 2.1481 (2.1481)	mem 19490MB
[2022-04-02 21:57:41 simmim_pretrain] (main_simmim.py 184): INFO Train: [29/100][100/625]	eta 0:06:58 lr 0.000651	time 0.7628 (0.7976)	loss 1.7321 (1.7610)	grad_norm 8.8883 (3.2409)	mem 19490MB
[2022-04-02 21:58:58 simmim_pretrain] (main_simmim.py 184): INFO Train: [29/100][200/625]	eta 0:05:32 lr 0.000650	time 0.7707 (0.7821)	loss 1.6447 (1.7330)	grad_norm 3.2579 (3.1793)	mem 19490MB
[2022-04-02 22:00:15 simmim_pretrain] (main_simmim.py 184): INFO Train: [29/100][300/625]	eta 0:04:12 lr 0.000648	time 0.7611 (0.7774)	loss 1.6193 (1.7088)	grad_norm 0.9954 (2.9791)	mem 19490MB
[2022-04-02 22:01:31 simmim_pretrain] (main_simmim.py 184): INFO Train: [29/100][400/625]	eta 0:02:54 lr 0.000647	time 0.7613 (0.7747)	loss 1.6059 (1.6891)	grad_norm 1.5997 (2.6968)	mem 19490MB
[2022-04-02 22:02:48 simmim_pretrain] (main_simmim.py 184): INFO Train: [29/100][500/625]	eta 0:01:36 lr 0.000645	time 0.7552 (0.7737)	loss 1.6582 (1.6745)	grad_norm 1.3365 (2.5157)	mem 19490MB
[2022-04-02 22:04:05 simmim_pretrain] (main_simmim.py 184): INFO Train: [29/100][600/625]	eta 0:00:19 lr 0.000644	time 0.7602 (0.7730)	loss 1.6000 (1.6634)	grad_norm 2.2216 (2.3460)	mem 19490MB
[2022-04-02 22:04:24 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 29 training takes 0:08:03
[2022-04-02 22:04:28 simmim_pretrain] (main_simmim.py 184): INFO Train: [30/100][0/625]	eta 0:39:34 lr 0.000643	time 3.7989 (3.7989)	loss 1.5831 (1.5831)	grad_norm 1.0828 (1.0828)	mem 19490MB
[2022-04-02 22:05:45 simmim_pretrain] (main_simmim.py 184): INFO Train: [30/100][100/625]	eta 0:06:59 lr 0.000642	time 0.7665 (0.7987)	loss 1.5971 (1.5957)	grad_norm 1.5793 (1.6292)	mem 19490MB
[2022-04-02 22:07:01 simmim_pretrain] (main_simmim.py 184): INFO Train: [30/100][200/625]	eta 0:05:32 lr 0.000640	time 0.7642 (0.7835)	loss 1.5733 (1.5931)	grad_norm 1.3915 (1.5709)	mem 19490MB
[2022-04-02 22:08:18 simmim_pretrain] (main_simmim.py 184): INFO Train: [30/100][300/625]	eta 0:04:12 lr 0.000639	time 0.7708 (0.7779)	loss 1.5758 (1.5927)	grad_norm 1.4552 (1.5594)	mem 19490MB
[2022-04-02 22:09:35 simmim_pretrain] (main_simmim.py 184): INFO Train: [30/100][400/625]	eta 0:02:54 lr 0.000637	time 0.7676 (0.7761)	loss 1.5460 (1.5912)	grad_norm 2.3746 (1.5472)	mem 19490MB
[2022-04-02 22:10:52 simmim_pretrain] (main_simmim.py 184): INFO Train: [30/100][500/625]	eta 0:01:36 lr 0.000636	time 0.7861 (0.7744)	loss 1.6013 (1.5907)	grad_norm 1.9150 (1.5742)	mem 19490MB
[2022-04-02 22:12:09 simmim_pretrain] (main_simmim.py 184): INFO Train: [30/100][600/625]	eta 0:00:19 lr 0.000634	time 0.7657 (0.7731)	loss 1.6182 (1.5892)	grad_norm 1.4594 (1.5558)	mem 19490MB
[2022-04-02 22:12:27 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 30 training takes 0:08:03
[2022-04-02 22:12:32 simmim_pretrain] (main_simmim.py 184): INFO Train: [31/100][0/625]	eta 0:49:45 lr 0.000634	time 4.7767 (4.7767)	loss 1.6109 (1.6109)	grad_norm 1.0824 (1.0824)	mem 19490MB
[2022-04-02 22:13:49 simmim_pretrain] (main_simmim.py 184): INFO Train: [31/100][100/625]	eta 0:07:03 lr 0.000632	time 0.7744 (0.8075)	loss 1.5544 (1.5814)	grad_norm 1.4252 (1.4025)	mem 19490MB
[2022-04-02 22:15:06 simmim_pretrain] (main_simmim.py 184): INFO Train: [31/100][200/625]	eta 0:05:34 lr 0.000630	time 0.7718 (0.7867)	loss 1.5651 (1.5791)	grad_norm 2.4063 (1.5281)	mem 19490MB
[2022-04-02 22:16:22 simmim_pretrain] (main_simmim.py 184): INFO Train: [31/100][300/625]	eta 0:04:13 lr 0.000629	time 0.7624 (0.7805)	loss 1.5909 (1.5822)	grad_norm 1.1531 (1.6442)	mem 19490MB
[2022-04-02 22:17:39 simmim_pretrain] (main_simmim.py 184): INFO Train: [31/100][400/625]	eta 0:02:54 lr 0.000627	time 0.7749 (0.7777)	loss 1.5638 (1.5810)	grad_norm 1.2503 (1.5887)	mem 19490MB
[2022-04-02 22:18:56 simmim_pretrain] (main_simmim.py 184): INFO Train: [31/100][500/625]	eta 0:01:36 lr 0.000626	time 0.7547 (0.7753)	loss 1.5491 (1.5806)	grad_norm 1.2961 (1.5396)	mem 19490MB
[2022-04-02 22:20:12 simmim_pretrain] (main_simmim.py 184): INFO Train: [31/100][600/625]	eta 0:00:19 lr 0.000624	time 0.7664 (0.7737)	loss 1.5508 (1.5796)	grad_norm 0.9886 (1.5148)	mem 19490MB
[2022-04-02 22:20:31 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 31 training takes 0:08:03
[2022-04-02 22:20:35 simmim_pretrain] (main_simmim.py 184): INFO Train: [32/100][0/625]	eta 0:40:32 lr 0.000624	time 3.8922 (3.8922)	loss 1.5766 (1.5766)	grad_norm 1.1625 (1.1625)	mem 19490MB
[2022-04-02 22:21:52 simmim_pretrain] (main_simmim.py 184): INFO Train: [32/100][100/625]	eta 0:06:59 lr 0.000622	time 0.7694 (0.7989)	loss 1.5956 (1.5745)	grad_norm 1.6822 (1.4687)	mem 19490MB
[2022-04-02 22:23:09 simmim_pretrain] (main_simmim.py 184): INFO Train: [32/100][200/625]	eta 0:05:32 lr 0.000620	time 0.7618 (0.7832)	loss 1.5609 (1.5725)	grad_norm 0.9442 (1.4938)	mem 19490MB
[2022-04-02 22:24:25 simmim_pretrain] (main_simmim.py 184): INFO Train: [32/100][300/625]	eta 0:04:12 lr 0.000619	time 0.7637 (0.7774)	loss 1.5810 (1.5722)	grad_norm 1.6167 (1.4911)	mem 19490MB
[2022-04-02 22:25:42 simmim_pretrain] (main_simmim.py 184): INFO Train: [32/100][400/625]	eta 0:02:54 lr 0.000617	time 0.7577 (0.7749)	loss 1.5140 (1.5723)	grad_norm 2.0485 (1.4614)	mem 19490MB
[2022-04-02 22:26:59 simmim_pretrain] (main_simmim.py 184): INFO Train: [32/100][500/625]	eta 0:01:36 lr 0.000615	time 0.7672 (0.7734)	loss 1.5699 (1.5743)	grad_norm 1.6313 (1.5416)	mem 19490MB
[2022-04-02 22:28:15 simmim_pretrain] (main_simmim.py 184): INFO Train: [32/100][600/625]	eta 0:00:19 lr 0.000614	time 0.7767 (0.7724)	loss 1.5752 (1.5745)	grad_norm 1.3841 (1.5201)	mem 19490MB
[2022-04-02 22:28:34 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 32 training takes 0:08:03
[2022-04-02 22:28:38 simmim_pretrain] (main_simmim.py 184): INFO Train: [33/100][0/625]	eta 0:39:01 lr 0.000613	time 3.7467 (3.7467)	loss 1.5677 (1.5677)	grad_norm 1.5705 (1.5705)	mem 19490MB
[2022-04-02 22:29:55 simmim_pretrain] (main_simmim.py 184): INFO Train: [33/100][100/625]	eta 0:07:00 lr 0.000612	time 0.7621 (0.8002)	loss 1.5656 (1.5730)	grad_norm 2.5019 (1.3692)	mem 19490MB
[2022-04-02 22:31:12 simmim_pretrain] (main_simmim.py 184): INFO Train: [33/100][200/625]	eta 0:05:33 lr 0.000610	time 0.7577 (0.7849)	loss 1.5626 (1.5727)	grad_norm 1.3650 (1.3890)	mem 19490MB
[2022-04-02 22:32:29 simmim_pretrain] (main_simmim.py 184): INFO Train: [33/100][300/625]	eta 0:04:13 lr 0.000608	time 0.7594 (0.7792)	loss 1.5382 (1.5719)	grad_norm 1.0478 (1.3922)	mem 19490MB
[2022-04-02 22:33:46 simmim_pretrain] (main_simmim.py 184): INFO Train: [33/100][400/625]	eta 0:02:54 lr 0.000607	time 0.7638 (0.7765)	loss 1.5571 (1.5713)	grad_norm 1.1877 (1.3981)	mem 19490MB
[2022-04-02 22:35:02 simmim_pretrain] (main_simmim.py 184): INFO Train: [33/100][500/625]	eta 0:01:36 lr 0.000605	time 0.7708 (0.7745)	loss 1.6088 (1.5708)	grad_norm 1.4854 (1.3916)	mem 19490MB
[2022-04-02 22:36:19 simmim_pretrain] (main_simmim.py 184): INFO Train: [33/100][600/625]	eta 0:00:19 lr 0.000603	time 0.7660 (0.7734)	loss 1.5492 (1.5696)	grad_norm 1.1724 (1.4079)	mem 19490MB
[2022-04-02 22:36:38 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 33 training takes 0:08:03
[2022-04-02 22:36:42 simmim_pretrain] (main_simmim.py 184): INFO Train: [34/100][0/625]	eta 0:39:12 lr 0.000603	time 3.7639 (3.7639)	loss 1.5669 (1.5669)	grad_norm 1.4509 (1.4509)	mem 19490MB
[2022-04-02 22:37:58 simmim_pretrain] (main_simmim.py 184): INFO Train: [34/100][100/625]	eta 0:06:58 lr 0.000601	time 0.7793 (0.7975)	loss 1.5393 (1.5652)	grad_norm 1.6538 (1.4806)	mem 19490MB
[2022-04-02 22:39:15 simmim_pretrain] (main_simmim.py 184): INFO Train: [34/100][200/625]	eta 0:05:32 lr 0.000600	time 0.7690 (0.7825)	loss 1.5731 (1.5656)	grad_norm 1.3649 (1.4651)	mem 19490MB
[2022-04-02 22:40:32 simmim_pretrain] (main_simmim.py 184): INFO Train: [34/100][300/625]	eta 0:04:12 lr 0.000598	time 0.7797 (0.7776)	loss 1.5835 (1.5789)	grad_norm 0.9648 (1.8002)	mem 19490MB
[2022-04-02 22:41:49 simmim_pretrain] (main_simmim.py 184): INFO Train: [34/100][400/625]	eta 0:02:54 lr 0.000596	time 0.7748 (0.7752)	loss 1.5845 (1.5767)	grad_norm 0.8861 (1.6200)	mem 19490MB
[2022-04-02 22:43:06 simmim_pretrain] (main_simmim.py 184): INFO Train: [34/100][500/625]	eta 0:01:36 lr 0.000595	time 0.7722 (0.7738)	loss 1.5656 (1.5751)	grad_norm 1.2972 (1.5380)	mem 19490MB
[2022-04-02 22:44:23 simmim_pretrain] (main_simmim.py 184): INFO Train: [34/100][600/625]	eta 0:00:19 lr 0.000593	time 0.7667 (0.7732)	loss 1.5672 (1.5732)	grad_norm 1.7974 (1.4757)	mem 19490MB
[2022-04-02 22:44:41 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 34 training takes 0:08:03
[2022-04-02 22:44:46 simmim_pretrain] (main_simmim.py 184): INFO Train: [35/100][0/625]	eta 0:44:56 lr 0.000593	time 4.3150 (4.3150)	loss 1.5518 (1.5518)	grad_norm 2.8316 (2.8316)	mem 19490MB
[2022-04-02 22:46:03 simmim_pretrain] (main_simmim.py 184): INFO Train: [35/100][100/625]	eta 0:07:01 lr 0.000591	time 0.7669 (0.8030)	loss 1.5592 (1.5659)	grad_norm 1.7119 (1.2625)	mem 19490MB
[2022-04-02 22:47:19 simmim_pretrain] (main_simmim.py 184): INFO Train: [35/100][200/625]	eta 0:05:33 lr 0.000589	time 0.7583 (0.7851)	loss 1.5547 (1.5646)	grad_norm 1.3096 (1.1914)	mem 19490MB
[2022-04-02 22:48:36 simmim_pretrain] (main_simmim.py 184): INFO Train: [35/100][300/625]	eta 0:04:13 lr 0.000587	time 0.7714 (0.7794)	loss 1.5665 (1.5645)	grad_norm 1.0700 (1.1897)	mem 19490MB
[2022-04-02 22:49:53 simmim_pretrain] (main_simmim.py 184): INFO Train: [35/100][400/625]	eta 0:02:54 lr 0.000586	time 0.7762 (0.7766)	loss 1.5704 (1.5639)	grad_norm 1.4428 (1.2047)	mem 19490MB
[2022-04-02 22:51:10 simmim_pretrain] (main_simmim.py 184): INFO Train: [35/100][500/625]	eta 0:01:36 lr 0.000584	time 0.7751 (0.7748)	loss 1.6001 (1.5636)	grad_norm 0.9888 (1.2298)	mem 19490MB
[2022-04-02 22:52:26 simmim_pretrain] (main_simmim.py 184): INFO Train: [35/100][600/625]	eta 0:00:19 lr 0.000582	time 0.7633 (0.7737)	loss 1.5788 (1.5635)	grad_norm 0.9506 (1.2603)	mem 19490MB
[2022-04-02 22:52:45 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 35 training takes 0:08:03
[2022-04-02 22:52:50 simmim_pretrain] (main_simmim.py 184): INFO Train: [36/100][0/625]	eta 0:48:40 lr 0.000582	time 4.6722 (4.6722)	loss 1.5181 (1.5181)	grad_norm 0.8305 (0.8305)	mem 19490MB
[2022-04-02 22:54:07 simmim_pretrain] (main_simmim.py 184): INFO Train: [36/100][100/625]	eta 0:07:04 lr 0.000580	time 0.7753 (0.8080)	loss 1.5557 (1.5608)	grad_norm 1.2448 (1.2618)	mem 19490MB
[2022-04-02 22:55:24 simmim_pretrain] (main_simmim.py 184): INFO Train: [36/100][200/625]	eta 0:05:34 lr 0.000578	time 0.7571 (0.7879)	loss 1.5678 (1.5816)	grad_norm 1.4237 (inf)	mem 19490MB
[2022-04-02 22:56:41 simmim_pretrain] (main_simmim.py 184): INFO Train: [36/100][300/625]	eta 0:04:14 lr 0.000577	time 0.7745 (0.7819)	loss 1.5880 (1.5765)	grad_norm 1.0770 (inf)	mem 19490MB
[2022-04-02 22:57:58 simmim_pretrain] (main_simmim.py 184): INFO Train: [36/100][400/625]	eta 0:02:55 lr 0.000575	time 0.7610 (0.7795)	loss 1.5495 (1.5750)	grad_norm 0.7911 (inf)	mem 19490MB
[2022-04-02 22:59:14 simmim_pretrain] (main_simmim.py 184): INFO Train: [36/100][500/625]	eta 0:01:37 lr 0.000573	time 0.7991 (0.7769)	loss 1.5462 (1.5727)	grad_norm 0.7476 (inf)	mem 19490MB
[2022-04-02 23:00:31 simmim_pretrain] (main_simmim.py 184): INFO Train: [36/100][600/625]	eta 0:00:19 lr 0.000571	time 0.7639 (0.7752)	loss 1.5760 (1.5700)	grad_norm 1.5810 (inf)	mem 19490MB
[2022-04-02 23:00:50 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 36 training takes 0:08:04
[2022-04-02 23:00:54 simmim_pretrain] (main_simmim.py 184): INFO Train: [37/100][0/625]	eta 0:45:26 lr 0.000571	time 4.3623 (4.3623)	loss 1.5316 (1.5316)	grad_norm 1.1962 (1.1962)	mem 19490MB
[2022-04-02 23:02:11 simmim_pretrain] (main_simmim.py 184): INFO Train: [37/100][100/625]	eta 0:07:02 lr 0.000569	time 0.7618 (0.8041)	loss 1.5668 (1.5570)	grad_norm 2.0573 (1.1572)	mem 19490MB
[2022-04-02 23:03:28 simmim_pretrain] (main_simmim.py 184): INFO Train: [37/100][200/625]	eta 0:05:33 lr 0.000567	time 0.7783 (0.7853)	loss 1.5948 (1.5576)	grad_norm 1.2562 (1.1396)	mem 19490MB
[2022-04-02 23:04:45 simmim_pretrain] (main_simmim.py 184): INFO Train: [37/100][300/625]	eta 0:04:13 lr 0.000566	time 0.7838 (0.7796)	loss 1.5414 (1.5575)	grad_norm 0.8084 (1.1462)	mem 19490MB
[2022-04-02 23:06:01 simmim_pretrain] (main_simmim.py 184): INFO Train: [37/100][400/625]	eta 0:02:54 lr 0.000564	time 0.7602 (0.7765)	loss 1.5215 (1.5572)	grad_norm 0.7780 (1.1494)	mem 19490MB
[2022-04-02 23:07:18 simmim_pretrain] (main_simmim.py 184): INFO Train: [37/100][500/625]	eta 0:01:36 lr 0.000562	time 0.7589 (0.7747)	loss 1.6052 (1.5575)	grad_norm 1.7609 (1.1686)	mem 19490MB
[2022-04-02 23:08:35 simmim_pretrain] (main_simmim.py 184): INFO Train: [37/100][600/625]	eta 0:00:19 lr 0.000560	time 0.7753 (0.7736)	loss 1.5459 (1.5569)	grad_norm 0.7933 (1.1781)	mem 19490MB
[2022-04-02 23:08:54 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 37 training takes 0:08:03
[2022-04-02 23:08:58 simmim_pretrain] (main_simmim.py 184): INFO Train: [38/100][0/625]	eta 0:41:13 lr 0.000560	time 3.9581 (3.9581)	loss 1.5737 (1.5737)	grad_norm 1.8062 (1.8062)	mem 19490MB
[2022-04-02 23:10:15 simmim_pretrain] (main_simmim.py 184): INFO Train: [38/100][100/625]	eta 0:07:00 lr 0.000558	time 0.7741 (0.8007)	loss 1.5235 (1.5549)	grad_norm 0.6786 (1.2397)	mem 19490MB
[2022-04-02 23:11:32 simmim_pretrain] (main_simmim.py 184): INFO Train: [38/100][200/625]	eta 0:05:33 lr 0.000556	time 0.7431 (0.7851)	loss 1.5319 (1.5582)	grad_norm 0.6497 (1.4135)	mem 19490MB
[2022-04-02 23:12:48 simmim_pretrain] (main_simmim.py 184): INFO Train: [38/100][300/625]	eta 0:04:13 lr 0.000555	time 0.7635 (0.7792)	loss 1.5804 (1.5577)	grad_norm 1.1099 (1.2869)	mem 19490MB
[2022-04-02 23:14:05 simmim_pretrain] (main_simmim.py 184): INFO Train: [38/100][400/625]	eta 0:02:54 lr 0.000553	time 0.7696 (0.7760)	loss 1.5594 (1.5563)	grad_norm 1.3998 (1.2398)	mem 19490MB
[2022-04-02 23:15:22 simmim_pretrain] (main_simmim.py 184): INFO Train: [38/100][500/625]	eta 0:01:36 lr 0.000551	time 0.7673 (0.7742)	loss 1.5519 (1.5561)	grad_norm 1.3206 (1.2429)	mem 19490MB
[2022-04-02 23:16:38 simmim_pretrain] (main_simmim.py 184): INFO Train: [38/100][600/625]	eta 0:00:19 lr 0.000549	time 0.7731 (0.7731)	loss 1.5617 (1.5559)	grad_norm 1.0833 (1.2179)	mem 19490MB
[2022-04-02 23:16:57 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 38 training takes 0:08:03
[2022-04-02 23:17:01 simmim_pretrain] (main_simmim.py 184): INFO Train: [39/100][0/625]	eta 0:39:51 lr 0.000549	time 3.8262 (3.8262)	loss 1.5450 (1.5450)	grad_norm 2.0074 (2.0074)	mem 19490MB
[2022-04-02 23:18:18 simmim_pretrain] (main_simmim.py 184): INFO Train: [39/100][100/625]	eta 0:06:58 lr 0.000547	time 0.7576 (0.7976)	loss 1.5732 (1.5544)	grad_norm 0.9293 (1.2028)	mem 19490MB
[2022-04-02 23:19:35 simmim_pretrain] (main_simmim.py 184): INFO Train: [39/100][200/625]	eta 0:05:32 lr 0.000545	time 0.7681 (0.7826)	loss 1.5111 (1.5550)	grad_norm 1.1893 (1.1164)	mem 19490MB
[2022-04-02 23:20:51 simmim_pretrain] (main_simmim.py 184): INFO Train: [39/100][300/625]	eta 0:04:12 lr 0.000543	time 0.7606 (0.7778)	loss 1.6026 (1.5538)	grad_norm 1.5501 (1.0980)	mem 19490MB
[2022-04-02 23:22:08 simmim_pretrain] (main_simmim.py 184): INFO Train: [39/100][400/625]	eta 0:02:54 lr 0.000542	time 0.7690 (0.7750)	loss 1.5389 (1.5539)	grad_norm 1.3509 (1.1071)	mem 19490MB
[2022-04-02 23:23:25 simmim_pretrain] (main_simmim.py 184): INFO Train: [39/100][500/625]	eta 0:01:36 lr 0.000540	time 0.7647 (0.7733)	loss 1.5414 (1.5538)	grad_norm 1.0816 (1.1097)	mem 19490MB
[2022-04-02 23:24:42 simmim_pretrain] (main_simmim.py 184): INFO Train: [39/100][600/625]	eta 0:00:19 lr 0.000538	time 0.7651 (0.7731)	loss 1.5683 (1.5534)	grad_norm 1.2915 (1.1350)	mem 19490MB
[2022-04-02 23:25:01 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 39 training takes 0:08:03
[2022-04-02 23:25:05 simmim_pretrain] (main_simmim.py 184): INFO Train: [40/100][0/625]	eta 0:46:31 lr 0.000537	time 4.4669 (4.4669)	loss 1.6117 (1.6117)	grad_norm 3.6616 (3.6616)	mem 19490MB
[2022-04-02 23:26:22 simmim_pretrain] (main_simmim.py 184): INFO Train: [40/100][100/625]	eta 0:07:02 lr 0.000536	time 0.7682 (0.8050)	loss 1.5645 (1.5615)	grad_norm 0.7172 (1.1888)	mem 19490MB
[2022-04-02 23:27:39 simmim_pretrain] (main_simmim.py 184): INFO Train: [40/100][200/625]	eta 0:05:34 lr 0.000534	time 0.7662 (0.7862)	loss 1.6019 (1.5597)	grad_norm 0.9489 (1.2209)	mem 19490MB
[2022-04-02 23:28:56 simmim_pretrain] (main_simmim.py 184): INFO Train: [40/100][300/625]	eta 0:04:13 lr 0.000532	time 0.7686 (0.7803)	loss 1.5724 (1.5574)	grad_norm 1.4204 (1.1305)	mem 19490MB
[2022-04-02 23:30:12 simmim_pretrain] (main_simmim.py 184): INFO Train: [40/100][400/625]	eta 0:02:54 lr 0.000530	time 0.7722 (0.7771)	loss 1.5318 (1.5553)	grad_norm 0.7450 (1.1091)	mem 19490MB
[2022-04-02 23:31:29 simmim_pretrain] (main_simmim.py 184): INFO Train: [40/100][500/625]	eta 0:01:36 lr 0.000528	time 0.7649 (0.7756)	loss 1.5310 (1.5544)	grad_norm 1.1156 (1.1008)	mem 19490MB
[2022-04-02 23:32:46 simmim_pretrain] (main_simmim.py 184): INFO Train: [40/100][600/625]	eta 0:00:19 lr 0.000526	time 0.7576 (0.7744)	loss 1.5629 (1.5536)	grad_norm 1.2066 (1.0908)	mem 19490MB
[2022-04-02 23:33:05 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 40 training takes 0:08:04
[2022-04-02 23:33:10 simmim_pretrain] (main_simmim.py 184): INFO Train: [41/100][0/625]	eta 0:49:37 lr 0.000526	time 4.7633 (4.7633)	loss 1.5395 (1.5395)	grad_norm 0.8413 (0.8413)	mem 19490MB
[2022-04-02 23:34:27 simmim_pretrain] (main_simmim.py 184): INFO Train: [41/100][100/625]	eta 0:07:03 lr 0.000524	time 0.7579 (0.8073)	loss 1.5247 (1.5508)	grad_norm 0.8677 (1.0819)	mem 19490MB
[2022-04-02 23:35:43 simmim_pretrain] (main_simmim.py 184): INFO Train: [41/100][200/625]	eta 0:05:34 lr 0.000522	time 0.7666 (0.7871)	loss 1.5017 (1.5483)	grad_norm 1.3926 (1.0446)	mem 19490MB
[2022-04-02 23:37:00 simmim_pretrain] (main_simmim.py 184): INFO Train: [41/100][300/625]	eta 0:04:13 lr 0.000521	time 0.7718 (0.7804)	loss 1.5406 (1.5522)	grad_norm 1.1501 (1.2052)	mem 19490MB
[2022-04-02 23:38:17 simmim_pretrain] (main_simmim.py 184): INFO Train: [41/100][400/625]	eta 0:02:55 lr 0.000519	time 0.7637 (0.7781)	loss 1.5763 (1.5510)	grad_norm 0.6363 (1.1584)	mem 19490MB
[2022-04-02 23:39:34 simmim_pretrain] (main_simmim.py 184): INFO Train: [41/100][500/625]	eta 0:01:37 lr 0.000517	time 0.7549 (0.7761)	loss 1.5172 (1.5501)	grad_norm 0.5311 (1.1311)	mem 19490MB
[2022-04-02 23:40:51 simmim_pretrain] (main_simmim.py 184): INFO Train: [41/100][600/625]	eta 0:00:19 lr 0.000515	time 0.7660 (0.7746)	loss 1.5437 (1.5503)	grad_norm 1.1953 (1.1167)	mem 19490MB
[2022-04-02 23:41:09 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 41 training takes 0:08:04
[2022-04-02 23:41:14 simmim_pretrain] (main_simmim.py 184): INFO Train: [42/100][0/625]	eta 0:43:15 lr 0.000515	time 4.1521 (4.1521)	loss 1.5476 (1.5476)	grad_norm 1.4611 (1.4611)	mem 19490MB
[2022-04-02 23:42:30 simmim_pretrain] (main_simmim.py 184): INFO Train: [42/100][100/625]	eta 0:07:00 lr 0.000513	time 0.7751 (0.8000)	loss 1.5329 (1.5474)	grad_norm 0.8748 (0.9788)	mem 19490MB
[2022-04-02 23:43:47 simmim_pretrain] (main_simmim.py 184): INFO Train: [42/100][200/625]	eta 0:05:32 lr 0.000511	time 0.7730 (0.7826)	loss 1.5488 (1.5601)	grad_norm 0.6712 (1.3807)	mem 19490MB
[2022-04-02 23:45:03 simmim_pretrain] (main_simmim.py 184): INFO Train: [42/100][300/625]	eta 0:04:12 lr 0.000509	time 0.7556 (0.7774)	loss 1.5255 (1.5555)	grad_norm 0.8859 (1.2346)	mem 19490MB
[2022-04-02 23:46:20 simmim_pretrain] (main_simmim.py 184): INFO Train: [42/100][400/625]	eta 0:02:54 lr 0.000507	time 0.7634 (0.7746)	loss 1.5321 (1.5540)	grad_norm 1.2230 (1.1669)	mem 19490MB
[2022-04-02 23:47:37 simmim_pretrain] (main_simmim.py 184): INFO Train: [42/100][500/625]	eta 0:01:36 lr 0.000505	time 0.7797 (0.7733)	loss 1.5487 (1.5518)	grad_norm 0.8566 (1.1350)	mem 19490MB
[2022-04-02 23:48:54 simmim_pretrain] (main_simmim.py 184): INFO Train: [42/100][600/625]	eta 0:00:19 lr 0.000503	time 0.7589 (0.7723)	loss 1.5522 (1.5506)	grad_norm 0.8489 (1.1098)	mem 19490MB
[2022-04-02 23:49:12 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 42 training takes 0:08:02
[2022-04-02 23:49:16 simmim_pretrain] (main_simmim.py 184): INFO Train: [43/100][0/625]	eta 0:41:43 lr 0.000503	time 4.0052 (4.0052)	loss 1.5584 (1.5584)	grad_norm 0.7940 (0.7940)	mem 19490MB
[2022-04-02 23:50:33 simmim_pretrain] (main_simmim.py 184): INFO Train: [43/100][100/625]	eta 0:07:00 lr 0.000501	time 0.7642 (0.8010)	loss 1.5530 (1.5457)	grad_norm 1.2790 (0.9444)	mem 19490MB
[2022-04-02 23:51:50 simmim_pretrain] (main_simmim.py 184): INFO Train: [43/100][200/625]	eta 0:05:33 lr 0.000499	time 0.7635 (0.7851)	loss 1.5654 (1.5461)	grad_norm 0.8480 (1.0015)	mem 19490MB
[2022-04-02 23:53:07 simmim_pretrain] (main_simmim.py 184): INFO Train: [43/100][300/625]	eta 0:04:13 lr 0.000497	time 0.7650 (0.7794)	loss 1.5273 (1.5450)	grad_norm 1.3777 (1.0105)	mem 19490MB
[2022-04-02 23:54:24 simmim_pretrain] (main_simmim.py 184): INFO Train: [43/100][400/625]	eta 0:02:54 lr 0.000495	time 0.7643 (0.7762)	loss 1.5517 (1.5446)	grad_norm 0.9143 (1.0172)	mem 19490MB
[2022-04-02 23:55:40 simmim_pretrain] (main_simmim.py 184): INFO Train: [43/100][500/625]	eta 0:01:36 lr 0.000494	time 0.7668 (0.7741)	loss 1.5714 (1.5440)	grad_norm 0.7625 (1.0075)	mem 19490MB
[2022-04-02 23:56:57 simmim_pretrain] (main_simmim.py 184): INFO Train: [43/100][600/625]	eta 0:00:19 lr 0.000492	time 0.7685 (0.7730)	loss 1.5668 (1.5438)	grad_norm 0.9351 (1.0043)	mem 19490MB
[2022-04-02 23:57:16 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 43 training takes 0:08:03
[2022-04-02 23:57:20 simmim_pretrain] (main_simmim.py 184): INFO Train: [44/100][0/625]	eta 0:43:52 lr 0.000491	time 4.2120 (4.2120)	loss 1.5439 (1.5439)	grad_norm 0.8465 (0.8465)	mem 19490MB
[2022-04-02 23:58:37 simmim_pretrain] (main_simmim.py 184): INFO Train: [44/100][100/625]	eta 0:07:01 lr 0.000489	time 0.7635 (0.8027)	loss 1.5380 (1.5385)	grad_norm 1.3508 (1.0203)	mem 19490MB
[2022-04-02 23:59:53 simmim_pretrain] (main_simmim.py 184): INFO Train: [44/100][200/625]	eta 0:05:33 lr 0.000487	time 0.7643 (0.7850)	loss 1.5452 (1.5420)	grad_norm 0.9783 (1.0159)	mem 19490MB
[2022-04-03 00:01:10 simmim_pretrain] (main_simmim.py 184): INFO Train: [44/100][300/625]	eta 0:04:13 lr 0.000486	time 0.7611 (0.7792)	loss 1.5804 (1.5418)	grad_norm 1.0956 (0.9927)	mem 19490MB
[2022-04-03 00:02:27 simmim_pretrain] (main_simmim.py 184): INFO Train: [44/100][400/625]	eta 0:02:54 lr 0.000484	time 0.7693 (0.7763)	loss 1.5504 (1.5419)	grad_norm 1.1361 (1.0197)	mem 19490MB
[2022-04-03 00:03:44 simmim_pretrain] (main_simmim.py 184): INFO Train: [44/100][500/625]	eta 0:01:36 lr 0.000482	time 0.7750 (0.7745)	loss 1.5503 (1.5421)	grad_norm 0.9083 (1.0207)	mem 19490MB
[2022-04-03 00:05:01 simmim_pretrain] (main_simmim.py 184): INFO Train: [44/100][600/625]	eta 0:00:19 lr 0.000480	time 0.7730 (0.7741)	loss 1.5521 (1.5427)	grad_norm 1.2364 (1.0350)	mem 19490MB
[2022-04-03 00:05:20 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 44 training takes 0:08:04
[2022-04-03 00:05:23 simmim_pretrain] (main_simmim.py 184): INFO Train: [45/100][0/625]	eta 0:39:11 lr 0.000479	time 3.7631 (3.7631)	loss 1.5218 (1.5218)	grad_norm 0.6979 (0.6979)	mem 19490MB
[2022-04-03 00:06:40 simmim_pretrain] (main_simmim.py 184): INFO Train: [45/100][100/625]	eta 0:06:59 lr 0.000478	time 0.7612 (0.7981)	loss 1.5254 (1.5442)	grad_norm 0.8537 (1.0915)	mem 19490MB
[2022-04-03 00:07:57 simmim_pretrain] (main_simmim.py 184): INFO Train: [45/100][200/625]	eta 0:05:32 lr 0.000476	time 0.7570 (0.7822)	loss 1.5199 (1.5440)	grad_norm 1.0988 (0.9882)	mem 19490MB
[2022-04-03 00:09:14 simmim_pretrain] (main_simmim.py 184): INFO Train: [45/100][300/625]	eta 0:04:12 lr 0.000474	time 0.7640 (0.7770)	loss 1.5376 (1.5427)	grad_norm 0.6661 (0.9978)	mem 19490MB
[2022-04-03 00:10:30 simmim_pretrain] (main_simmim.py 184): INFO Train: [45/100][400/625]	eta 0:02:54 lr 0.000472	time 0.7726 (0.7742)	loss 1.5219 (1.5426)	grad_norm 1.1172 (1.0138)	mem 19490MB
[2022-04-03 00:11:47 simmim_pretrain] (main_simmim.py 184): INFO Train: [45/100][500/625]	eta 0:01:36 lr 0.000470	time 0.7660 (0.7728)	loss 1.5596 (1.5421)	grad_norm 1.3636 (1.0259)	mem 19490MB
[2022-04-03 00:13:04 simmim_pretrain] (main_simmim.py 184): INFO Train: [45/100][600/625]	eta 0:00:19 lr 0.000468	time 0.7587 (0.7719)	loss 1.5678 (1.5419)	grad_norm 0.7682 (inf)	mem 19490MB
[2022-04-03 00:13:22 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 45 training takes 0:08:02
[2022-04-03 00:13:28 simmim_pretrain] (main_simmim.py 184): INFO Train: [46/100][0/625]	eta 0:55:14 lr 0.000468	time 5.3036 (5.3036)	loss 1.5367 (1.5367)	grad_norm 0.8112 (0.8112)	mem 19490MB
[2022-04-03 00:14:45 simmim_pretrain] (main_simmim.py 184): INFO Train: [46/100][100/625]	eta 0:07:06 lr 0.000466	time 0.7672 (0.8133)	loss 1.5496 (1.5400)	grad_norm 1.1780 (1.0857)	mem 19490MB
[2022-04-03 00:16:01 simmim_pretrain] (main_simmim.py 184): INFO Train: [46/100][200/625]	eta 0:05:35 lr 0.000464	time 0.7520 (0.7900)	loss 1.5472 (1.5380)	grad_norm 1.1613 (1.0310)	mem 19490MB
[2022-04-03 00:17:18 simmim_pretrain] (main_simmim.py 184): INFO Train: [46/100][300/625]	eta 0:04:14 lr 0.000462	time 0.7711 (0.7833)	loss 1.5323 (1.5376)	grad_norm 0.8726 (1.0219)	mem 19490MB
[2022-04-03 00:18:35 simmim_pretrain] (main_simmim.py 184): INFO Train: [46/100][400/625]	eta 0:02:55 lr 0.000460	time 0.7744 (0.7804)	loss 1.5646 (1.5374)	grad_norm 1.0881 (0.9999)	mem 19490MB
[2022-04-03 00:19:52 simmim_pretrain] (main_simmim.py 184): INFO Train: [46/100][500/625]	eta 0:01:37 lr 0.000458	time 0.7492 (0.7781)	loss 1.5624 (1.5429)	grad_norm 0.7780 (inf)	mem 19490MB
[2022-04-03 00:21:09 simmim_pretrain] (main_simmim.py 184): INFO Train: [46/100][600/625]	eta 0:00:19 lr 0.000456	time 0.7728 (0.7762)	loss 1.5612 (1.5421)	grad_norm 0.6470 (inf)	mem 19490MB
[2022-04-03 00:21:28 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 46 training takes 0:08:05
[2022-04-03 00:21:32 simmim_pretrain] (main_simmim.py 184): INFO Train: [47/100][0/625]	eta 0:39:18 lr 0.000456	time 3.7734 (3.7734)	loss 1.5355 (1.5355)	grad_norm 1.0070 (1.0070)	mem 19490MB
[2022-04-03 00:22:48 simmim_pretrain] (main_simmim.py 184): INFO Train: [47/100][100/625]	eta 0:06:58 lr 0.000454	time 0.7662 (0.7963)	loss 1.5459 (1.5384)	grad_norm 0.7231 (0.8466)	mem 19490MB
[2022-04-03 00:24:05 simmim_pretrain] (main_simmim.py 184): INFO Train: [47/100][200/625]	eta 0:05:31 lr 0.000452	time 0.7683 (0.7805)	loss 1.5265 (1.5385)	grad_norm 0.8678 (0.8670)	mem 19490MB
[2022-04-03 00:25:21 simmim_pretrain] (main_simmim.py 184): INFO Train: [47/100][300/625]	eta 0:04:12 lr 0.000450	time 0.7751 (0.7760)	loss 1.5277 (1.5382)	grad_norm 0.7480 (0.8785)	mem 19490MB
[2022-04-03 00:26:38 simmim_pretrain] (main_simmim.py 184): INFO Train: [47/100][400/625]	eta 0:02:54 lr 0.000448	time 0.7752 (0.7737)	loss 1.5178 (1.5382)	grad_norm 1.0282 (0.9064)	mem 19490MB
[2022-04-03 00:27:55 simmim_pretrain] (main_simmim.py 184): INFO Train: [47/100][500/625]	eta 0:01:36 lr 0.000446	time 0.7717 (0.7723)	loss 1.5625 (1.5386)	grad_norm 1.0808 (0.9092)	mem 19490MB
[2022-04-03 00:29:11 simmim_pretrain] (main_simmim.py 184): INFO Train: [47/100][600/625]	eta 0:00:19 lr 0.000444	time 0.7715 (0.7715)	loss 1.5215 (1.5383)	grad_norm 0.8429 (0.9170)	mem 19490MB
[2022-04-03 00:29:30 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 47 training takes 0:08:02
[2022-04-03 00:29:34 simmim_pretrain] (main_simmim.py 184): INFO Train: [48/100][0/625]	eta 0:38:54 lr 0.000444	time 3.7351 (3.7351)	loss 1.5523 (1.5523)	grad_norm 1.1666 (1.1666)	mem 19490MB
[2022-04-03 00:30:51 simmim_pretrain] (main_simmim.py 184): INFO Train: [48/100][100/625]	eta 0:06:59 lr 0.000442	time 0.7753 (0.7981)	loss 1.5361 (1.5340)	grad_norm 0.5627 (0.9408)	mem 19490MB
[2022-04-03 00:32:08 simmim_pretrain] (main_simmim.py 184): INFO Train: [48/100][200/625]	eta 0:05:33 lr 0.000440	time 0.7689 (0.7841)	loss 1.5300 (1.5340)	grad_norm 1.4785 (0.9353)	mem 19490MB
[2022-04-03 00:33:25 simmim_pretrain] (main_simmim.py 184): INFO Train: [48/100][300/625]	eta 0:04:13 lr 0.000438	time 0.7710 (0.7789)	loss 1.5174 (1.5339)	grad_norm 0.9075 (0.9494)	mem 19490MB
[2022-04-03 00:34:41 simmim_pretrain] (main_simmim.py 184): INFO Train: [48/100][400/625]	eta 0:02:54 lr 0.000436	time 0.7595 (0.7762)	loss 1.5580 (1.5344)	grad_norm 0.8008 (0.9364)	mem 19490MB
[2022-04-03 00:35:58 simmim_pretrain] (main_simmim.py 184): INFO Train: [48/100][500/625]	eta 0:01:36 lr 0.000434	time 0.7647 (0.7746)	loss 1.5426 (1.5348)	grad_norm 0.7693 (0.9312)	mem 19490MB
[2022-04-03 00:37:15 simmim_pretrain] (main_simmim.py 184): INFO Train: [48/100][600/625]	eta 0:00:19 lr 0.000432	time 0.7741 (0.7732)	loss 1.5259 (1.5350)	grad_norm 0.6248 (0.9420)	mem 19490MB
[2022-04-03 00:37:34 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 48 training takes 0:08:03
[2022-04-03 00:37:38 simmim_pretrain] (main_simmim.py 184): INFO Train: [49/100][0/625]	eta 0:41:58 lr 0.000432	time 4.0291 (4.0291)	loss 1.5286 (1.5286)	grad_norm 0.8027 (0.8027)	mem 19490MB
[2022-04-03 00:38:55 simmim_pretrain] (main_simmim.py 184): INFO Train: [49/100][100/625]	eta 0:07:00 lr 0.000430	time 0.7589 (0.8012)	loss 1.5215 (1.5411)	grad_norm 0.8992 (1.2264)	mem 19490MB
[2022-04-03 00:40:11 simmim_pretrain] (main_simmim.py 184): INFO Train: [49/100][200/625]	eta 0:05:33 lr 0.000428	time 0.7703 (0.7838)	loss 1.5587 (1.5397)	grad_norm 1.2152 (1.0692)	mem 19490MB
[2022-04-03 00:41:28 simmim_pretrain] (main_simmim.py 184): INFO Train: [49/100][300/625]	eta 0:04:12 lr 0.000426	time 0.7721 (0.7782)	loss 1.5285 (1.5376)	grad_norm 1.0510 (1.0590)	mem 19490MB
[2022-04-03 00:42:45 simmim_pretrain] (main_simmim.py 184): INFO Train: [49/100][400/625]	eta 0:02:54 lr 0.000424	time 0.7556 (0.7752)	loss 1.5158 (1.5366)	grad_norm 1.1354 (1.0545)	mem 19490MB
[2022-04-03 00:44:01 simmim_pretrain] (main_simmim.py 184): INFO Train: [49/100][500/625]	eta 0:01:36 lr 0.000422	time 0.7625 (0.7735)	loss 1.5785 (1.5363)	grad_norm 1.2215 (1.0354)	mem 19490MB
[2022-04-03 00:45:18 simmim_pretrain] (main_simmim.py 184): INFO Train: [49/100][600/625]	eta 0:00:19 lr 0.000420	time 0.7651 (0.7726)	loss 1.5153 (1.5363)	grad_norm 0.6635 (1.0057)	mem 19490MB
[2022-04-03 00:45:37 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 49 training takes 0:08:02
[2022-04-03 00:45:41 simmim_pretrain] (main_simmim.py 184): INFO Train: [50/100][0/625]	eta 0:42:55 lr 0.000420	time 4.1205 (4.1205)	loss 1.5584 (1.5584)	grad_norm 0.9649 (0.9649)	mem 19490MB
[2022-04-03 00:46:58 simmim_pretrain] (main_simmim.py 184): INFO Train: [50/100][100/625]	eta 0:07:01 lr 0.000418	time 0.7590 (0.8024)	loss 1.4912 (1.5331)	grad_norm 1.2494 (0.9075)	mem 19490MB
[2022-04-03 00:48:14 simmim_pretrain] (main_simmim.py 184): INFO Train: [50/100][200/625]	eta 0:05:33 lr 0.000416	time 0.7664 (0.7847)	loss 1.5481 (1.5341)	grad_norm 0.9333 (0.9134)	mem 19490MB
[2022-04-03 00:49:31 simmim_pretrain] (main_simmim.py 184): INFO Train: [50/100][300/625]	eta 0:04:13 lr 0.000414	time 0.7763 (0.7791)	loss 1.5313 (1.5335)	grad_norm 0.6582 (0.9061)	mem 19490MB
[2022-04-03 00:50:48 simmim_pretrain] (main_simmim.py 184): INFO Train: [50/100][400/625]	eta 0:02:54 lr 0.000412	time 0.7734 (0.7761)	loss 1.5186 (1.5329)	grad_norm 0.6112 (0.8951)	mem 19490MB
[2022-04-03 00:52:04 simmim_pretrain] (main_simmim.py 184): INFO Train: [50/100][500/625]	eta 0:01:36 lr 0.000410	time 0.7600 (0.7740)	loss 1.5317 (1.5328)	grad_norm 1.3406 (0.9161)	mem 19490MB
[2022-04-03 00:53:21 simmim_pretrain] (main_simmim.py 184): INFO Train: [50/100][600/625]	eta 0:00:19 lr 0.000409	time 0.7665 (0.7727)	loss 1.5411 (1.5326)	grad_norm 0.8486 (0.8973)	mem 19490MB
[2022-04-03 00:53:40 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 50 training takes 0:08:03
[2022-04-03 00:53:45 simmim_pretrain] (main_simmim.py 184): INFO Train: [51/100][0/625]	eta 0:49:45 lr 0.000408	time 4.7765 (4.7765)	loss 1.5319 (1.5319)	grad_norm 0.8156 (0.8156)	mem 19490MB
[2022-04-03 00:55:02 simmim_pretrain] (main_simmim.py 184): INFO Train: [51/100][100/625]	eta 0:07:03 lr 0.000406	time 0.7615 (0.8072)	loss 1.5088 (1.5329)	grad_norm 0.6817 (0.8991)	mem 19490MB
[2022-04-03 00:56:18 simmim_pretrain] (main_simmim.py 184): INFO Train: [51/100][200/625]	eta 0:05:34 lr 0.000404	time 0.7634 (0.7873)	loss 1.5453 (1.5339)	grad_norm 0.9608 (0.9196)	mem 19490MB
[2022-04-03 00:57:35 simmim_pretrain] (main_simmim.py 184): INFO Train: [51/100][300/625]	eta 0:04:13 lr 0.000402	time 0.7775 (0.7810)	loss 1.5471 (1.5322)	grad_norm 0.6609 (0.8761)	mem 19490MB
[2022-04-03 00:58:52 simmim_pretrain] (main_simmim.py 184): INFO Train: [51/100][400/625]	eta 0:02:55 lr 0.000400	time 0.7543 (0.7778)	loss 1.5106 (1.5315)	grad_norm 1.1140 (0.8972)	mem 19490MB
[2022-04-03 01:00:09 simmim_pretrain] (main_simmim.py 184): INFO Train: [51/100][500/625]	eta 0:01:37 lr 0.000399	time 0.7764 (0.7761)	loss 1.5586 (1.5310)	grad_norm 1.1951 (0.8905)	mem 19490MB
[2022-04-03 01:01:26 simmim_pretrain] (main_simmim.py 184): INFO Train: [51/100][600/625]	eta 0:00:19 lr 0.000397	time 0.7643 (0.7748)	loss 1.5303 (1.5318)	grad_norm 0.6234 (0.9089)	mem 19490MB
[2022-04-03 01:01:44 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 51 training takes 0:08:04
[2022-04-03 01:01:48 simmim_pretrain] (main_simmim.py 184): INFO Train: [52/100][0/625]	eta 0:40:00 lr 0.000396	time 3.8402 (3.8402)	loss 1.5083 (1.5083)	grad_norm 0.8385 (0.8385)	mem 19490MB
[2022-04-03 01:03:05 simmim_pretrain] (main_simmim.py 184): INFO Train: [52/100][100/625]	eta 0:06:59 lr 0.000394	time 0.7463 (0.7983)	loss 1.5802 (1.5334)	grad_norm inf (inf)	mem 19490MB
[2022-04-03 01:04:22 simmim_pretrain] (main_simmim.py 184): INFO Train: [52/100][200/625]	eta 0:05:32 lr 0.000392	time 0.7711 (0.7823)	loss 1.5650 (1.5376)	grad_norm 0.7310 (inf)	mem 19490MB
[2022-04-03 01:05:38 simmim_pretrain] (main_simmim.py 184): INFO Train: [52/100][300/625]	eta 0:04:12 lr 0.000390	time 0.7601 (0.7773)	loss 1.5363 (1.5352)	grad_norm 0.5879 (inf)	mem 19490MB
[2022-04-03 01:06:55 simmim_pretrain] (main_simmim.py 184): INFO Train: [52/100][400/625]	eta 0:02:54 lr 0.000389	time 0.7736 (0.7748)	loss 1.5302 (1.5340)	grad_norm 0.8260 (inf)	mem 19490MB
[2022-04-03 01:08:12 simmim_pretrain] (main_simmim.py 184): INFO Train: [52/100][500/625]	eta 0:01:36 lr 0.000387	time 0.7674 (0.7733)	loss 1.5222 (1.5332)	grad_norm 0.8192 (inf)	mem 19490MB
[2022-04-03 01:09:29 simmim_pretrain] (main_simmim.py 184): INFO Train: [52/100][600/625]	eta 0:00:19 lr 0.000385	time 0.7635 (0.7723)	loss 1.5429 (1.5326)	grad_norm 0.8408 (inf)	mem 19490MB
[2022-04-03 01:09:47 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 52 training takes 0:08:03
[2022-04-03 01:09:51 simmim_pretrain] (main_simmim.py 184): INFO Train: [53/100][0/625]	eta 0:39:10 lr 0.000384	time 3.7606 (3.7606)	loss 1.5374 (1.5374)	grad_norm 0.9118 (0.9118)	mem 19490MB
[2022-04-03 01:11:08 simmim_pretrain] (main_simmim.py 184): INFO Train: [53/100][100/625]	eta 0:06:58 lr 0.000382	time 0.7754 (0.7969)	loss 1.5244 (1.5267)	grad_norm 0.7291 (0.9311)	mem 19490MB
[2022-04-03 01:12:25 simmim_pretrain] (main_simmim.py 184): INFO Train: [53/100][200/625]	eta 0:05:32 lr 0.000380	time 0.7784 (0.7828)	loss 1.5149 (1.5257)	grad_norm 0.6336 (0.8759)	mem 19490MB
[2022-04-03 01:13:42 simmim_pretrain] (main_simmim.py 184): INFO Train: [53/100][300/625]	eta 0:04:12 lr 0.000379	time 0.7655 (0.7782)	loss 1.5028 (1.5271)	grad_norm 0.7178 (0.8721)	mem 19490MB
[2022-04-03 01:14:58 simmim_pretrain] (main_simmim.py 184): INFO Train: [53/100][400/625]	eta 0:02:54 lr 0.000377	time 0.7635 (0.7756)	loss 1.5045 (1.5278)	grad_norm 1.1711 (0.8778)	mem 19490MB
[2022-04-03 01:16:15 simmim_pretrain] (main_simmim.py 184): INFO Train: [53/100][500/625]	eta 0:01:36 lr 0.000375	time 0.7756 (0.7738)	loss 1.5649 (1.5278)	grad_norm 1.0907 (0.8809)	mem 19490MB
[2022-04-03 01:17:32 simmim_pretrain] (main_simmim.py 184): INFO Train: [53/100][600/625]	eta 0:00:19 lr 0.000373	time 0.7658 (0.7726)	loss 1.5487 (1.5280)	grad_norm 0.9357 (0.8775)	mem 19490MB
[2022-04-03 01:17:51 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 53 training takes 0:08:03
[2022-04-03 01:17:54 simmim_pretrain] (main_simmim.py 184): INFO Train: [54/100][0/625]	eta 0:39:54 lr 0.000372	time 3.8306 (3.8306)	loss 1.5167 (1.5167)	grad_norm 0.4969 (0.4969)	mem 19490MB
[2022-04-03 01:19:11 simmim_pretrain] (main_simmim.py 184): INFO Train: [54/100][100/625]	eta 0:06:59 lr 0.000370	time 0.7694 (0.7985)	loss 1.5134 (1.5257)	grad_norm 0.7479 (0.8462)	mem 19490MB
[2022-04-03 01:20:28 simmim_pretrain] (main_simmim.py 184): INFO Train: [54/100][200/625]	eta 0:05:32 lr 0.000369	time 0.7664 (0.7828)	loss 1.5562 (1.5274)	grad_norm 1.3747 (0.8265)	mem 19490MB
[2022-04-03 01:21:45 simmim_pretrain] (main_simmim.py 184): INFO Train: [54/100][300/625]	eta 0:04:12 lr 0.000367	time 0.7779 (0.7777)	loss 1.5235 (1.5288)	grad_norm 1.2781 (0.8445)	mem 19490MB
[2022-04-03 01:23:01 simmim_pretrain] (main_simmim.py 184): INFO Train: [54/100][400/625]	eta 0:02:54 lr 0.000365	time 0.7760 (0.7751)	loss 1.5042 (1.5282)	grad_norm 0.9627 (0.8593)	mem 19490MB
[2022-04-03 01:24:18 simmim_pretrain] (main_simmim.py 184): INFO Train: [54/100][500/625]	eta 0:01:36 lr 0.000363	time 0.7660 (0.7736)	loss 1.5030 (1.5276)	grad_norm 0.6034 (0.8584)	mem 19490MB
[2022-04-03 01:25:35 simmim_pretrain] (main_simmim.py 184): INFO Train: [54/100][600/625]	eta 0:00:19 lr 0.000361	time 0.7727 (0.7725)	loss 1.5403 (1.5277)	grad_norm 0.9459 (0.8585)	mem 19490MB
[2022-04-03 01:25:54 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 54 training takes 0:08:03
[2022-04-03 01:25:58 simmim_pretrain] (main_simmim.py 184): INFO Train: [55/100][0/625]	eta 0:38:56 lr 0.000361	time 3.7384 (3.7384)	loss 1.5256 (1.5256)	grad_norm 0.9133 (0.9133)	mem 19490MB
[2022-04-03 01:27:15 simmim_pretrain] (main_simmim.py 184): INFO Train: [55/100][100/625]	eta 0:06:59 lr 0.000359	time 0.7644 (0.7988)	loss 1.5128 (1.5243)	grad_norm 0.9713 (0.8214)	mem 19490MB
[2022-04-03 01:28:31 simmim_pretrain] (main_simmim.py 184): INFO Train: [55/100][200/625]	eta 0:05:32 lr 0.000357	time 0.7662 (0.7827)	loss 1.5543 (1.5234)	grad_norm 1.0446 (0.8363)	mem 19490MB
[2022-04-03 01:29:48 simmim_pretrain] (main_simmim.py 184): INFO Train: [55/100][300/625]	eta 0:04:12 lr 0.000355	time 0.7819 (0.7777)	loss 1.5564 (1.5256)	grad_norm 0.6582 (0.8534)	mem 19490MB
[2022-04-03 01:31:05 simmim_pretrain] (main_simmim.py 184): INFO Train: [55/100][400/625]	eta 0:02:54 lr 0.000353	time 0.7596 (0.7750)	loss 1.5126 (1.5260)	grad_norm 0.9170 (0.8436)	mem 19490MB
[2022-04-03 01:32:21 simmim_pretrain] (main_simmim.py 184): INFO Train: [55/100][500/625]	eta 0:01:36 lr 0.000351	time 0.7711 (0.7732)	loss 1.5434 (1.5258)	grad_norm 0.9987 (0.8671)	mem 19490MB
[2022-04-03 01:33:38 simmim_pretrain] (main_simmim.py 184): INFO Train: [55/100][600/625]	eta 0:00:19 lr 0.000349	time 0.7611 (0.7721)	loss 1.5300 (1.5254)	grad_norm 0.6730 (0.8727)	mem 19490MB
[2022-04-03 01:33:57 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 55 training takes 0:08:02
[2022-04-03 01:34:02 simmim_pretrain] (main_simmim.py 184): INFO Train: [56/100][0/625]	eta 0:56:42 lr 0.000349	time 5.4443 (5.4443)	loss 1.5170 (1.5170)	grad_norm 0.5674 (0.5674)	mem 19490MB
[2022-04-03 01:35:19 simmim_pretrain] (main_simmim.py 184): INFO Train: [56/100][100/625]	eta 0:07:08 lr 0.000347	time 0.7760 (0.8166)	loss 1.5242 (1.5239)	grad_norm 0.6585 (0.8038)	mem 19490MB
[2022-04-03 01:36:36 simmim_pretrain] (main_simmim.py 184): INFO Train: [56/100][200/625]	eta 0:05:36 lr 0.000345	time 0.7679 (0.7923)	loss 1.5381 (1.5262)	grad_norm 1.0013 (0.8144)	mem 19490MB
[2022-04-03 01:37:53 simmim_pretrain] (main_simmim.py 184): INFO Train: [56/100][300/625]	eta 0:04:14 lr 0.000343	time 0.7716 (0.7841)	loss 1.5268 (1.5253)	grad_norm 0.8570 (0.8179)	mem 19490MB
[2022-04-03 01:39:10 simmim_pretrain] (main_simmim.py 184): INFO Train: [56/100][400/625]	eta 0:02:55 lr 0.000341	time 0.7666 (0.7800)	loss 1.5781 (1.5265)	grad_norm 2.4023 (inf)	mem 19490MB
[2022-04-03 01:40:27 simmim_pretrain] (main_simmim.py 184): INFO Train: [56/100][500/625]	eta 0:01:37 lr 0.000339	time 0.7828 (0.7780)	loss 1.5275 (1.5270)	grad_norm 0.7681 (inf)	mem 19490MB
[2022-04-03 01:41:43 simmim_pretrain] (main_simmim.py 184): INFO Train: [56/100][600/625]	eta 0:00:19 lr 0.000338	time 0.7702 (0.7765)	loss 1.5189 (1.5268)	grad_norm 0.7800 (inf)	mem 19490MB
[2022-04-03 01:42:02 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 56 training takes 0:08:05
[2022-04-03 01:42:06 simmim_pretrain] (main_simmim.py 184): INFO Train: [57/100][0/625]	eta 0:42:34 lr 0.000337	time 4.0870 (4.0870)	loss 1.5046 (1.5046)	grad_norm 0.6525 (0.6525)	mem 19490MB
[2022-04-03 01:43:23 simmim_pretrain] (main_simmim.py 184): INFO Train: [57/100][100/625]	eta 0:07:00 lr 0.000335	time 0.7716 (0.8009)	loss 1.5297 (1.5216)	grad_norm 0.7190 (0.7625)	mem 19490MB
[2022-04-03 01:44:40 simmim_pretrain] (main_simmim.py 184): INFO Train: [57/100][200/625]	eta 0:05:33 lr 0.000333	time 0.7679 (0.7840)	loss 1.5042 (1.5235)	grad_norm 0.8367 (0.8130)	mem 19490MB
[2022-04-03 01:45:57 simmim_pretrain] (main_simmim.py 184): INFO Train: [57/100][300/625]	eta 0:04:12 lr 0.000332	time 0.7601 (0.7783)	loss 1.5037 (1.5234)	grad_norm 0.6899 (0.8218)	mem 19490MB
[2022-04-03 01:47:13 simmim_pretrain] (main_simmim.py 184): INFO Train: [57/100][400/625]	eta 0:02:54 lr 0.000330	time 0.7590 (0.7757)	loss 1.5051 (1.5230)	grad_norm 1.1439 (0.8088)	mem 19490MB
[2022-04-03 01:48:30 simmim_pretrain] (main_simmim.py 184): INFO Train: [57/100][500/625]	eta 0:01:36 lr 0.000328	time 0.7637 (0.7741)	loss 1.5098 (1.5224)	grad_norm 1.0514 (0.8099)	mem 19490MB
[2022-04-03 01:49:47 simmim_pretrain] (main_simmim.py 184): INFO Train: [57/100][600/625]	eta 0:00:19 lr 0.000326	time 0.7682 (0.7728)	loss 1.5698 (1.5230)	grad_norm 1.0056 (0.8099)	mem 19490MB
[2022-04-03 01:50:06 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 57 training takes 0:08:03
[2022-04-03 01:50:09 simmim_pretrain] (main_simmim.py 184): INFO Train: [58/100][0/625]	eta 0:39:30 lr 0.000325	time 3.7926 (3.7926)	loss 1.5307 (1.5307)	grad_norm 0.7884 (0.7884)	mem 19490MB
[2022-04-03 01:51:26 simmim_pretrain] (main_simmim.py 184): INFO Train: [58/100][100/625]	eta 0:06:58 lr 0.000324	time 0.7681 (0.7973)	loss 1.5195 (1.5234)	grad_norm 0.7900 (0.8115)	mem 19490MB
[2022-04-03 01:52:43 simmim_pretrain] (main_simmim.py 184): INFO Train: [58/100][200/625]	eta 0:05:32 lr 0.000322	time 0.7624 (0.7826)	loss 1.5365 (1.5235)	grad_norm 0.6176 (0.8444)	mem 19490MB
[2022-04-03 01:54:00 simmim_pretrain] (main_simmim.py 184): INFO Train: [58/100][300/625]	eta 0:04:13 lr 0.000320	time 0.7749 (0.7789)	loss 1.5013 (1.5224)	grad_norm 0.7701 (0.8386)	mem 19490MB
[2022-04-03 01:55:17 simmim_pretrain] (main_simmim.py 184): INFO Train: [58/100][400/625]	eta 0:02:54 lr 0.000318	time 0.7710 (0.7768)	loss 1.5118 (1.5226)	grad_norm 0.9574 (0.8175)	mem 19490MB
[2022-04-03 01:56:34 simmim_pretrain] (main_simmim.py 184): INFO Train: [58/100][500/625]	eta 0:01:36 lr 0.000316	time 0.7588 (0.7751)	loss 1.5509 (1.5219)	grad_norm 0.5782 (0.8183)	mem 19490MB
[2022-04-03 01:57:51 simmim_pretrain] (main_simmim.py 184): INFO Train: [58/100][600/625]	eta 0:00:19 lr 0.000314	time 0.7553 (0.7739)	loss 1.5227 (1.5218)	grad_norm 0.6949 (0.8149)	mem 19490MB
[2022-04-03 01:58:10 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 58 training takes 0:08:03
[2022-04-03 01:58:14 simmim_pretrain] (main_simmim.py 184): INFO Train: [59/100][0/625]	eta 0:40:38 lr 0.000314	time 3.9009 (3.9009)	loss 1.5372 (1.5372)	grad_norm 0.7340 (0.7340)	mem 19490MB
[2022-04-03 01:59:31 simmim_pretrain] (main_simmim.py 184): INFO Train: [59/100][100/625]	eta 0:07:00 lr 0.000312	time 0.7708 (0.8013)	loss 1.5277 (1.5215)	grad_norm 0.6682 (0.8256)	mem 19490MB
[2022-04-03 02:00:47 simmim_pretrain] (main_simmim.py 184): INFO Train: [59/100][200/625]	eta 0:05:33 lr 0.000310	time 0.7642 (0.7841)	loss 1.4967 (1.5213)	grad_norm 0.7497 (0.8241)	mem 19490MB
[2022-04-03 02:02:04 simmim_pretrain] (main_simmim.py 184): INFO Train: [59/100][300/625]	eta 0:04:13 lr 0.000308	time 0.7714 (0.7789)	loss 1.5326 (1.5213)	grad_norm 1.1740 (0.8412)	mem 19490MB
[2022-04-03 02:03:21 simmim_pretrain] (main_simmim.py 184): INFO Train: [59/100][400/625]	eta 0:02:54 lr 0.000307	time 0.7640 (0.7758)	loss 1.5433 (1.5210)	grad_norm 0.6794 (0.8347)	mem 19490MB
[2022-04-03 02:04:38 simmim_pretrain] (main_simmim.py 184): INFO Train: [59/100][500/625]	eta 0:01:36 lr 0.000305	time 0.7666 (0.7741)	loss 1.5201 (1.5206)	grad_norm 0.7249 (0.8233)	mem 19490MB
[2022-04-03 02:05:54 simmim_pretrain] (main_simmim.py 184): INFO Train: [59/100][600/625]	eta 0:00:19 lr 0.000303	time 0.7722 (0.7729)	loss 1.5146 (1.5203)	grad_norm 0.8125 (0.8180)	mem 19490MB
[2022-04-03 02:06:13 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 59 training takes 0:08:03
[2022-04-03 02:06:17 simmim_pretrain] (main_simmim.py 184): INFO Train: [60/100][0/625]	eta 0:45:07 lr 0.000303	time 4.3327 (4.3327)	loss 1.4923 (1.4923)	grad_norm 0.5113 (0.5113)	mem 19490MB
[2022-04-03 02:07:35 simmim_pretrain] (main_simmim.py 184): INFO Train: [60/100][100/625]	eta 0:07:04 lr 0.000301	time 0.7692 (0.8085)	loss 1.5090 (1.5193)	grad_norm 0.9545 (0.7751)	mem 19490MB
[2022-04-03 02:08:51 simmim_pretrain] (main_simmim.py 184): INFO Train: [60/100][200/625]	eta 0:05:34 lr 0.000299	time 0.7685 (0.7880)	loss 1.4860 (1.5191)	grad_norm 0.7641 (0.7961)	mem 19490MB
[2022-04-03 02:10:08 simmim_pretrain] (main_simmim.py 184): INFO Train: [60/100][300/625]	eta 0:04:14 lr 0.000297	time 0.7763 (0.7818)	loss 1.5213 (1.5193)	grad_norm 0.6270 (0.7951)	mem 19490MB
[2022-04-03 02:11:25 simmim_pretrain] (main_simmim.py 184): INFO Train: [60/100][400/625]	eta 0:02:55 lr 0.000295	time 0.7747 (0.7785)	loss 1.5153 (1.5190)	grad_norm 0.7403 (0.7933)	mem 19490MB
[2022-04-03 02:12:42 simmim_pretrain] (main_simmim.py 184): INFO Train: [60/100][500/625]	eta 0:01:37 lr 0.000294	time 0.7648 (0.7765)	loss 1.5319 (1.5188)	grad_norm 0.7976 (0.7894)	mem 19490MB
[2022-04-03 02:13:59 simmim_pretrain] (main_simmim.py 184): INFO Train: [60/100][600/625]	eta 0:00:19 lr 0.000292	time 0.7698 (0.7751)	loss 1.4990 (1.5186)	grad_norm 0.8706 (0.7996)	mem 19490MB
[2022-04-03 02:14:18 simmim_pretrain] (main_simmim.py 192): INFO EPOCH 60 training takes 0:08:04
[2022-04-03 02:14:22 simmim_pretrain] (main_simmim.py 184): INFO Train: [61/100][0/625]	eta 0:48:49 lr 0.000291	time 4.6878 (4.6878)	loss 1.5084 (1.5084)	grad_norm 0.8517 (0.8517)	mem 19490MB
[2022-04-03 02:15:39 simmim_pretrain] (main_simmim.py 184): INFO Train: [61/100][100/625]	eta 0:07:04 lr 0.000289	time 0.7824 (0.8083)	loss 1.4847 (1.5210)	grad_norm 0.8251 (0.8184)	mem 19490MB
[2022-04-03 02:16:56 simmim_pretrain] (main_simmim.py 184): INFO Train: [61/100][200/625]	eta 0:05:34 lr 0.000288	time 0.7708 (0.7879)	loss 1.5312 (1.5216)	grad_norm 0.7051 (0.7913)	mem 19490MB
[2022-04-03 02:18:13 simmim_pretrain] (main_simmim.py 184): INFO Train: [61/100][300/625]	eta 0:04:13 lr 0.000286	time 0.7707 (0.7815)	loss 1.5123 (1.5209)	grad_norm 0.7813 (0.7933)	mem 19490MB
[2022-04-03 02:19:30 simmim_pretrain] (main_simmim.py 184): INFO Train: [61/100][400/625]	eta 0:02:55 lr 0.000284	time 0.7591 (0.7785)	loss 1.5109 (1.5197)	grad_norm 0.6421 (0.7776)	mem 19490MB
[2022-04-03 14:06:32 simmim_pretrain] (main_simmim.py 242): INFO AMP_OPT_LEVEL: O1
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 256
  DATASET: imagenet
  DATA_PATH: /root/zhouyuchen/imagenet-1000/ILSVRC/Data/CLS-LOC//train
  IMG_SIZE: 192
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
EVAL_MODE: false
LOCAL_RANK: 6
MODEL:
  DROP_PATH_RATE: 0.0
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: simmim_pretrain
  NUM_CLASSES: 1000
  RESUME: ''
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 6
  TYPE: swin
  VIT:
    DEPTH: 12
    EMBED_DIM: 768
    INIT_VALUES: 0.1
    IN_CHANS: 3
    MLP_RATIO: 4
    NUM_HEADS: 12
    PATCH_SIZE: 16
    QKV_BIAS: true
    USE_APE: false
    USE_MEAN_POOLING: false
    USE_RPB: false
    USE_SHARED_RPB: true
OUTPUT: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep
PRETRAINED: ''
PRINT_FREQ: 100
SAVE_FREQ: 5
SEED: 0
TAG: simmim_pretrain__swin_base__img192_window6__100ep
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 100
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
  MIN_LR: 4.0e-05
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 10
  WARMUP_LR: 4.0e-06
  WEIGHT_DECAY: 0.05

[2022-04-03 14:06:32 simmim_pretrain] (data_simmim.py 96): INFO Pre-train data transform:
<data.data_simmim.SimMIMTransform object at 0x7f9186711a90>
[2022-04-03 14:06:36 simmim_pretrain] (data_simmim.py 99): INFO Build dataset: train images = 1281167
[2022-04-03 14:06:36 simmim_pretrain] (main_simmim.py 71): INFO Creating model:swin/simmim_pretrain
[2022-04-03 14:06:37 simmim_pretrain] (main_simmim.py 74): INFO SimMIM(
  (encoder): SwinTransformerForSimMIM(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (layers): ModuleList(
      (0): BasicLayer(
        dim=96, input_resolution=(48, 48), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=96, input_resolution=(48, 48), num_heads=3, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=96, window_size=(6, 6), num_heads=3
              (qkv): Linear(in_features=96, out_features=288, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=96, out_features=96, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=96, out_features=384, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=384, out_features=96, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (1): BasicLayer(
        dim=192, input_resolution=(24, 24), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=192, input_resolution=(24, 24), num_heads=6, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=192, window_size=(6, 6), num_heads=6
              (qkv): Linear(in_features=192, out_features=576, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=192, out_features=192, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=192, out_features=768, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=768, out_features=192, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(24, 24), dim=192
          (reduction): Linear(in_features=384, out_features=192, bias=False)
          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (2): BasicLayer(
        dim=384, input_resolution=(12, 12), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=384, input_resolution=(12, 12), num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=384, window_size=(6, 6), num_heads=12
              (qkv): Linear(in_features=384, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=384, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=384, out_features=1536, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=1536, out_features=384, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(12, 12), dim=384
          (reduction): Linear(in_features=768, out_features=384, bias=False)
          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (3): BasicLayer(
        dim=768, input_resolution=(6, 6), depth=2
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=768, input_resolution=(6, 6), num_heads=24, window_size=6, shift_size=0, mlp_ratio=4.0
            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=768, window_size=(6, 6), num_heads=24
              (qkv): Linear(in_features=768, out_features=2304, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=768, out_features=768, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (downsample): PatchMerging(
          input_resolution=(6, 6), dim=768
          (reduction): Linear(in_features=1536, out_features=768, bias=False)
          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (norm): ModuleList(
      (0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (avgpool): AdaptiveAvgPool1d(output_size=1)
    (head): Identity()
  )
  (decoder_stage_4): Conv2d(768, 27, kernel_size=(1, 1), stride=(1, 1))
  (decoder_stage_2): Conv2d(192, 27, kernel_size=(1, 1), stride=(1, 1))
  (down_sample): Linear(in_features=384, out_features=192, bias=True)
  (neck): BasicLayer(
    dim=192, input_resolution=[12, 12], depth=6
    (blocks): ModuleList(
      (0): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): SwinTransformerBlock(
        dim=192, input_resolution=[12, 12], num_heads=12, window_size=6, shift_size=3, mlp_ratio=4.0
        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=192, window_size=(6, 6), num_heads=12
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate=none)
          (fc2): Linear(in_features=768, out_features=192, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
)
[2022-04-03 14:06:37 simmim_pretrain] (optimizer.py 22): INFO >>>>>>>>>> Build Optimizer for Pre-training Stage
[2022-04-03 14:06:37 simmim_pretrain] (optimizer.py 27): INFO No weight decay: {'encoder.mask_token', 'encoder.absolute_pos_embed'}
[2022-04-03 14:06:37 simmim_pretrain] (optimizer.py 30): INFO No weight decay keywords: {'encoder.relative_position_bias_table'}
[2022-04-03 14:06:37 simmim_pretrain] (optimizer.py 63): INFO No decay params: ['encoder.mask_token', 'encoder.patch_embed.proj.bias', 'encoder.patch_embed.norm.weight', 'encoder.patch_embed.norm.bias', 'encoder.layers.0.blocks.0.norm1.weight', 'encoder.layers.0.blocks.0.norm1.bias', 'encoder.layers.0.blocks.0.attn.qkv.bias', 'encoder.layers.0.blocks.0.attn.proj.bias', 'encoder.layers.0.blocks.0.norm2.weight', 'encoder.layers.0.blocks.0.norm2.bias', 'encoder.layers.0.blocks.0.mlp.fc1.bias', 'encoder.layers.0.blocks.0.mlp.fc2.bias', 'encoder.layers.0.blocks.1.norm1.weight', 'encoder.layers.0.blocks.1.norm1.bias', 'encoder.layers.0.blocks.1.attn.qkv.bias', 'encoder.layers.0.blocks.1.attn.proj.bias', 'encoder.layers.0.blocks.1.norm2.weight', 'encoder.layers.0.blocks.1.norm2.bias', 'encoder.layers.0.blocks.1.mlp.fc1.bias', 'encoder.layers.0.blocks.1.mlp.fc2.bias', 'encoder.layers.1.blocks.0.norm1.weight', 'encoder.layers.1.blocks.0.norm1.bias', 'encoder.layers.1.blocks.0.attn.qkv.bias', 'encoder.layers.1.blocks.0.attn.proj.bias', 'encoder.layers.1.blocks.0.norm2.weight', 'encoder.layers.1.blocks.0.norm2.bias', 'encoder.layers.1.blocks.0.mlp.fc1.bias', 'encoder.layers.1.blocks.0.mlp.fc2.bias', 'encoder.layers.1.blocks.1.norm1.weight', 'encoder.layers.1.blocks.1.norm1.bias', 'encoder.layers.1.blocks.1.attn.qkv.bias', 'encoder.layers.1.blocks.1.attn.proj.bias', 'encoder.layers.1.blocks.1.norm2.weight', 'encoder.layers.1.blocks.1.norm2.bias', 'encoder.layers.1.blocks.1.mlp.fc1.bias', 'encoder.layers.1.blocks.1.mlp.fc2.bias', 'encoder.layers.1.downsample.norm.weight', 'encoder.layers.1.downsample.norm.bias', 'encoder.layers.2.blocks.0.norm1.weight', 'encoder.layers.2.blocks.0.norm1.bias', 'encoder.layers.2.blocks.0.attn.qkv.bias', 'encoder.layers.2.blocks.0.attn.proj.bias', 'encoder.layers.2.blocks.0.norm2.weight', 'encoder.layers.2.blocks.0.norm2.bias', 'encoder.layers.2.blocks.0.mlp.fc1.bias', 'encoder.layers.2.blocks.0.mlp.fc2.bias', 'encoder.layers.2.blocks.1.norm1.weight', 'encoder.layers.2.blocks.1.norm1.bias', 'encoder.layers.2.blocks.1.attn.qkv.bias', 'encoder.layers.2.blocks.1.attn.proj.bias', 'encoder.layers.2.blocks.1.norm2.weight', 'encoder.layers.2.blocks.1.norm2.bias', 'encoder.layers.2.blocks.1.mlp.fc1.bias', 'encoder.layers.2.blocks.1.mlp.fc2.bias', 'encoder.layers.2.blocks.2.norm1.weight', 'encoder.layers.2.blocks.2.norm1.bias', 'encoder.layers.2.blocks.2.attn.qkv.bias', 'encoder.layers.2.blocks.2.attn.proj.bias', 'encoder.layers.2.blocks.2.norm2.weight', 'encoder.layers.2.blocks.2.norm2.bias', 'encoder.layers.2.blocks.2.mlp.fc1.bias', 'encoder.layers.2.blocks.2.mlp.fc2.bias', 'encoder.layers.2.blocks.3.norm1.weight', 'encoder.layers.2.blocks.3.norm1.bias', 'encoder.layers.2.blocks.3.attn.qkv.bias', 'encoder.layers.2.blocks.3.attn.proj.bias', 'encoder.layers.2.blocks.3.norm2.weight', 'encoder.layers.2.blocks.3.norm2.bias', 'encoder.layers.2.blocks.3.mlp.fc1.bias', 'encoder.layers.2.blocks.3.mlp.fc2.bias', 'encoder.layers.2.blocks.4.norm1.weight', 'encoder.layers.2.blocks.4.norm1.bias', 'encoder.layers.2.blocks.4.attn.qkv.bias', 'encoder.layers.2.blocks.4.attn.proj.bias', 'encoder.layers.2.blocks.4.norm2.weight', 'encoder.layers.2.blocks.4.norm2.bias', 'encoder.layers.2.blocks.4.mlp.fc1.bias', 'encoder.layers.2.blocks.4.mlp.fc2.bias', 'encoder.layers.2.blocks.5.norm1.weight', 'encoder.layers.2.blocks.5.norm1.bias', 'encoder.layers.2.blocks.5.attn.qkv.bias', 'encoder.layers.2.blocks.5.attn.proj.bias', 'encoder.layers.2.blocks.5.norm2.weight', 'encoder.layers.2.blocks.5.norm2.bias', 'encoder.layers.2.blocks.5.mlp.fc1.bias', 'encoder.layers.2.blocks.5.mlp.fc2.bias', 'encoder.layers.2.downsample.norm.weight', 'encoder.layers.2.downsample.norm.bias', 'encoder.layers.3.blocks.0.norm1.weight', 'encoder.layers.3.blocks.0.norm1.bias', 'encoder.layers.3.blocks.0.attn.qkv.bias', 'encoder.layers.3.blocks.0.attn.proj.bias', 'encoder.layers.3.blocks.0.norm2.weight', 'encoder.layers.3.blocks.0.norm2.bias', 'encoder.layers.3.blocks.0.mlp.fc1.bias', 'encoder.layers.3.blocks.0.mlp.fc2.bias', 'encoder.layers.3.blocks.1.norm1.weight', 'encoder.layers.3.blocks.1.norm1.bias', 'encoder.layers.3.blocks.1.attn.qkv.bias', 'encoder.layers.3.blocks.1.attn.proj.bias', 'encoder.layers.3.blocks.1.norm2.weight', 'encoder.layers.3.blocks.1.norm2.bias', 'encoder.layers.3.blocks.1.mlp.fc1.bias', 'encoder.layers.3.blocks.1.mlp.fc2.bias', 'encoder.layers.3.downsample.norm.weight', 'encoder.layers.3.downsample.norm.bias', 'encoder.norm.0.weight', 'encoder.norm.0.bias', 'encoder.norm.1.weight', 'encoder.norm.1.bias', 'encoder.norm.2.weight', 'encoder.norm.2.bias', 'encoder.norm.3.weight', 'encoder.norm.3.bias', 'decoder_stage_4.bias', 'decoder_stage_2.bias', 'down_sample.bias', 'neck.blocks.0.norm1.weight', 'neck.blocks.0.norm1.bias', 'neck.blocks.0.attn.qkv.bias', 'neck.blocks.0.attn.proj.bias', 'neck.blocks.0.norm2.weight', 'neck.blocks.0.norm2.bias', 'neck.blocks.0.mlp.fc1.bias', 'neck.blocks.0.mlp.fc2.bias', 'neck.blocks.1.norm1.weight', 'neck.blocks.1.norm1.bias', 'neck.blocks.1.attn.qkv.bias', 'neck.blocks.1.attn.proj.bias', 'neck.blocks.1.norm2.weight', 'neck.blocks.1.norm2.bias', 'neck.blocks.1.mlp.fc1.bias', 'neck.blocks.1.mlp.fc2.bias', 'neck.blocks.2.norm1.weight', 'neck.blocks.2.norm1.bias', 'neck.blocks.2.attn.qkv.bias', 'neck.blocks.2.attn.proj.bias', 'neck.blocks.2.norm2.weight', 'neck.blocks.2.norm2.bias', 'neck.blocks.2.mlp.fc1.bias', 'neck.blocks.2.mlp.fc2.bias', 'neck.blocks.3.norm1.weight', 'neck.blocks.3.norm1.bias', 'neck.blocks.3.attn.qkv.bias', 'neck.blocks.3.attn.proj.bias', 'neck.blocks.3.norm2.weight', 'neck.blocks.3.norm2.bias', 'neck.blocks.3.mlp.fc1.bias', 'neck.blocks.3.mlp.fc2.bias', 'neck.blocks.4.norm1.weight', 'neck.blocks.4.norm1.bias', 'neck.blocks.4.attn.qkv.bias', 'neck.blocks.4.attn.proj.bias', 'neck.blocks.4.norm2.weight', 'neck.blocks.4.norm2.bias', 'neck.blocks.4.mlp.fc1.bias', 'neck.blocks.4.mlp.fc2.bias', 'neck.blocks.5.norm1.weight', 'neck.blocks.5.norm1.bias', 'neck.blocks.5.attn.qkv.bias', 'neck.blocks.5.attn.proj.bias', 'neck.blocks.5.norm2.weight', 'neck.blocks.5.norm2.bias', 'neck.blocks.5.mlp.fc1.bias', 'neck.blocks.5.mlp.fc2.bias']
[2022-04-03 14:06:37 simmim_pretrain] (optimizer.py 64): INFO Has decay params: ['encoder.patch_embed.proj.weight', 'encoder.layers.0.blocks.0.attn.relative_position_bias_table', 'encoder.layers.0.blocks.0.attn.qkv.weight', 'encoder.layers.0.blocks.0.attn.proj.weight', 'encoder.layers.0.blocks.0.mlp.fc1.weight', 'encoder.layers.0.blocks.0.mlp.fc2.weight', 'encoder.layers.0.blocks.1.attn.relative_position_bias_table', 'encoder.layers.0.blocks.1.attn.qkv.weight', 'encoder.layers.0.blocks.1.attn.proj.weight', 'encoder.layers.0.blocks.1.mlp.fc1.weight', 'encoder.layers.0.blocks.1.mlp.fc2.weight', 'encoder.layers.1.blocks.0.attn.relative_position_bias_table', 'encoder.layers.1.blocks.0.attn.qkv.weight', 'encoder.layers.1.blocks.0.attn.proj.weight', 'encoder.layers.1.blocks.0.mlp.fc1.weight', 'encoder.layers.1.blocks.0.mlp.fc2.weight', 'encoder.layers.1.blocks.1.attn.relative_position_bias_table', 'encoder.layers.1.blocks.1.attn.qkv.weight', 'encoder.layers.1.blocks.1.attn.proj.weight', 'encoder.layers.1.blocks.1.mlp.fc1.weight', 'encoder.layers.1.blocks.1.mlp.fc2.weight', 'encoder.layers.1.downsample.reduction.weight', 'encoder.layers.2.blocks.0.attn.relative_position_bias_table', 'encoder.layers.2.blocks.0.attn.qkv.weight', 'encoder.layers.2.blocks.0.attn.proj.weight', 'encoder.layers.2.blocks.0.mlp.fc1.weight', 'encoder.layers.2.blocks.0.mlp.fc2.weight', 'encoder.layers.2.blocks.1.attn.relative_position_bias_table', 'encoder.layers.2.blocks.1.attn.qkv.weight', 'encoder.layers.2.blocks.1.attn.proj.weight', 'encoder.layers.2.blocks.1.mlp.fc1.weight', 'encoder.layers.2.blocks.1.mlp.fc2.weight', 'encoder.layers.2.blocks.2.attn.relative_position_bias_table', 'encoder.layers.2.blocks.2.attn.qkv.weight', 'encoder.layers.2.blocks.2.attn.proj.weight', 'encoder.layers.2.blocks.2.mlp.fc1.weight', 'encoder.layers.2.blocks.2.mlp.fc2.weight', 'encoder.layers.2.blocks.3.attn.relative_position_bias_table', 'encoder.layers.2.blocks.3.attn.qkv.weight', 'encoder.layers.2.blocks.3.attn.proj.weight', 'encoder.layers.2.blocks.3.mlp.fc1.weight', 'encoder.layers.2.blocks.3.mlp.fc2.weight', 'encoder.layers.2.blocks.4.attn.relative_position_bias_table', 'encoder.layers.2.blocks.4.attn.qkv.weight', 'encoder.layers.2.blocks.4.attn.proj.weight', 'encoder.layers.2.blocks.4.mlp.fc1.weight', 'encoder.layers.2.blocks.4.mlp.fc2.weight', 'encoder.layers.2.blocks.5.attn.relative_position_bias_table', 'encoder.layers.2.blocks.5.attn.qkv.weight', 'encoder.layers.2.blocks.5.attn.proj.weight', 'encoder.layers.2.blocks.5.mlp.fc1.weight', 'encoder.layers.2.blocks.5.mlp.fc2.weight', 'encoder.layers.2.downsample.reduction.weight', 'encoder.layers.3.blocks.0.attn.relative_position_bias_table', 'encoder.layers.3.blocks.0.attn.qkv.weight', 'encoder.layers.3.blocks.0.attn.proj.weight', 'encoder.layers.3.blocks.0.mlp.fc1.weight', 'encoder.layers.3.blocks.0.mlp.fc2.weight', 'encoder.layers.3.blocks.1.attn.relative_position_bias_table', 'encoder.layers.3.blocks.1.attn.qkv.weight', 'encoder.layers.3.blocks.1.attn.proj.weight', 'encoder.layers.3.blocks.1.mlp.fc1.weight', 'encoder.layers.3.blocks.1.mlp.fc2.weight', 'encoder.layers.3.downsample.reduction.weight', 'decoder_stage_4.weight', 'decoder_stage_2.weight', 'down_sample.weight', 'neck.blocks.0.attn.relative_position_bias_table', 'neck.blocks.0.attn.qkv.weight', 'neck.blocks.0.attn.proj.weight', 'neck.blocks.0.mlp.fc1.weight', 'neck.blocks.0.mlp.fc2.weight', 'neck.blocks.1.attn.relative_position_bias_table', 'neck.blocks.1.attn.qkv.weight', 'neck.blocks.1.attn.proj.weight', 'neck.blocks.1.mlp.fc1.weight', 'neck.blocks.1.mlp.fc2.weight', 'neck.blocks.2.attn.relative_position_bias_table', 'neck.blocks.2.attn.qkv.weight', 'neck.blocks.2.attn.proj.weight', 'neck.blocks.2.mlp.fc1.weight', 'neck.blocks.2.mlp.fc2.weight', 'neck.blocks.3.attn.relative_position_bias_table', 'neck.blocks.3.attn.qkv.weight', 'neck.blocks.3.attn.proj.weight', 'neck.blocks.3.mlp.fc1.weight', 'neck.blocks.3.mlp.fc2.weight', 'neck.blocks.4.attn.relative_position_bias_table', 'neck.blocks.4.attn.qkv.weight', 'neck.blocks.4.attn.proj.weight', 'neck.blocks.4.mlp.fc1.weight', 'neck.blocks.4.mlp.fc2.weight', 'neck.blocks.5.attn.relative_position_bias_table', 'neck.blocks.5.attn.qkv.weight', 'neck.blocks.5.attn.proj.weight', 'neck.blocks.5.mlp.fc1.weight', 'neck.blocks.5.mlp.fc2.weight']
[2022-04-03 14:06:37 simmim_pretrain] (optimizer.py 43): INFO AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.05

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0008
    maximize: False
    weight_decay: 0.0
)
[2022-04-03 14:06:37 simmim_pretrain] (main_simmim.py 83): INFO number of params: 30291960
[2022-04-03 14:06:37 simmim_pretrain] (utils.py 81): INFO All checkpoints founded in output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep: ['ckpt_epoch_0.pth', 'ckpt_epoch_5.pth', 'ckpt_epoch_10.pth', 'ckpt_epoch_15.pth', 'ckpt_epoch_20.pth', 'ckpt_epoch_25.pth', 'ckpt_epoch_30.pth', 'ckpt_epoch_35.pth', 'ckpt_epoch_40.pth', 'ckpt_epoch_45.pth', 'ckpt_epoch_50.pth', 'ckpt_epoch_55.pth', 'ckpt_epoch_60.pth']
[2022-04-03 14:06:37 simmim_pretrain] (utils.py 84): INFO The latest checkpoint founded: output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep/ckpt_epoch_60.pth
[2022-04-03 14:06:37 simmim_pretrain] (main_simmim.py 98): INFO auto resuming from output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep/ckpt_epoch_60.pth
[2022-04-03 14:06:37 simmim_pretrain] (utils.py 23): INFO >>>>>>>>>> Resuming from output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep/ckpt_epoch_60.pth ..........
[2022-04-03 14:06:41 simmim_pretrain] (utils.py 30): INFO <All keys matched successfully>
[2022-04-03 14:06:41 simmim_pretrain] (utils.py 40): INFO => loaded successfully 'output/simmim_pretrain/simmim_pretrain__swin_base__img192_window6__100ep/ckpt_epoch_60.pth' (epoch 60)
[2022-04-03 14:06:41 simmim_pretrain] (main_simmim.py 105): INFO Start training
[2022-04-03 14:06:49 simmim_pretrain] (main_simmim.py 184): INFO Train: [61/100][0/625]	eta 1:26:22 lr 0.000291	time 8.2921 (8.2921)	loss 1.5175 (1.5175)	grad_norm 0.8857 (0.8857)	mem 19372MB
